{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":"<p>The WE1S Workspace is a containerized environment for importing, managing, and analyzing textual data using a variety of built-in analysis and visualization tools. It is designed to be well-documented and easy to use, but extendible, code-based environment. In the WE1S Workspace, users interact with their data inside a project created from a template. The latest version of the template can be downloaded from its Github repo.</p> <p>Version Compatibility</p> <p>This documentation is compatible with v1.0.0 of the WE1S Workspace and v1.0.0 of the WE1S project template.</p> <p>Projects are managed primarily through a Jupyter notebooks-based environment. Jupyter notebooks are a web-based interface for running code inside your web browser. If you are unfamiliar with Jupyter notebooks or Python, please read the the How Jupyter Notebooks Work and Introduction to Python sections of this guide before moving on to Creating a Project. These tutorials provide  basic introductions to how to use Jupyter notebooks and some basic Python terminology useful for navigating the WE1S Workspace.</p> <p>This website is designed to get you up and running once you have installed the Workspace on your computer.</p> <p>Important</p> <p>The search function on this site requires a server environment. If you copy the <code>getting_started</code> folder to a location outside the Workspace, you should still be able to read this site, but the search function will not work.</p> <p>For more information about the WhatEvery1Says (WE1S) Project can be found on the WE1S website: https://we1s.ucsb.edu/.</p> <p>Last update: 2023-06-14</p>"},{"location":"about/","title":"About the WhatEvery1Says (WE1S) Project","text":"<p>For more information about the WhatEvery1Says (WE1S) Project, go to https://we1s.ucsb.edu/. The WE1S GitHub site is https://github.com/whatevery1says.</p>"},{"location":"creating-a-project/","title":"Creating a Project","text":"<p>Begin by double-clicking on the <code>new_project.ipynb</code> notebook. Follow the instructions to create a new project, and then click the link to navigate to the project folder.</p> <p>Note</p> <p>It is good practice to close and halt the <code>new_project.ipynb</code> notebook after you have opened the project folder in a new browser tab.</p> <p>A project is a folder containing copies of the WE1S project template files and folders. Here is a brief description of each part of the project:</p>"},{"location":"creating-a-project/#the-readmemd-file","title":"The <code>README.md</code> File","text":"<p>This is a Markdown file containing details about the version of the WE1S template used to create the project and any metadata about the project which you added in <code>new_project.ipynb</code>. It is meant to be a human-readable guide to the content of the project.</p>"},{"location":"creating-a-project/#the-datapackagejson-file","title":"The <code>datapackage.json</code> File","text":"<p>The <code>datapackage.json</code> file is a manifest of your project's resources which is compliant with the WE1S manifest schema and the Frictionless Data project specification. The purpose of this file is to enable easier interoperability between your data and tools outside the WE1S Workspace. The <code>datapackage.json</code> file is is a JSON file containing metadata about the project and a complete list of the file paths to all resources in the project. If you export your project, the <code>export</code> module will detect any files you have added and add their paths to <code>datapackage.json</code>.</p>"},{"location":"creating-a-project/#the-configpy-file","title":"The <code>config.py</code> File","text":"<p>The <code>config.py</code> file inside the <code>config</code> folder is a Python file that contains information about the Workspace's server environment and the resources installed with the project template. It is used to restore the project to a virgin state if you run the <code>models/clear_caches.ipynb</code> notebook.</p>"},{"location":"creating-a-project/#the-modules-folder","title":"The <code>modules</code> Folder","text":"<p>The <code>modules</code> folder contains all the project's Jupyter notebooks (and supporting scripts and files). Each module focuses on a particular task: e.g., creating a topic model or visualizing it. Some modules can be used at any point in your workflow. Others need to be implemented in a certain order. For instance, the <code>dfr_browser</code>, <code>topic_bubbles</code>, and <code>pyldavis</code> modules create visualizations of topic models, so they will naturally not work until you have run the <code>topic_modeling</code> module.</p>"},{"location":"creating-a-project/#the-project_data-folder","title":"The <code>project_data</code> Folder","text":"<p>The <code>project_data</code> contains all your project's primary data. It is empty when the project is first created until you import your data using the <code>import</code> module.</p> <p>Note</p> <p>Sample projects providing example of the contents of each of these components of the Workspace can be found in the examples folder on GitHub.</p>"},{"location":"creating-a-project/#importing-data-to-your-project","title":"Importing Data to Your Project","text":"<p>Before you do anything else, you must import some data to your project. The WE1S accepts data in four formats:</p> <ol> <li>A zip archive of plain text (<code>.txt</code>) files accompanied by a CSV file containing metadata.</li> <li>A zip archive of JSON files combining the textual content and metadata.</li> <li>A Frictionless Data data package containing paths to all your data files.</li> <li>A query of records in a MongoDB database.</li> </ol> <p>To import your data, navigate to <code>modules/import/import.ipynb</code>. This notebook creates a new folder, <code>project_data/json</code> and copies your data from its source into this folder, converting it into JSON format, if necessary.</p> <p>Important</p> <p>Most tools in the WE1S Workspace use the JSON files in the <code>project_data/json</code> folder. These tools assume that the files are compliant with the WE1S manifest schema. The <code>import</code> module provides some functions for converting your metadata fields to the expected format; however, it cannot cover every scenario. You may need to perform some preprocessing on your data prior to import.</p> <p>Note</p> <p>The <code>import</code> module automatically coerces your textual data to UTF-8 character encoding.</p>"},{"location":"creating-a-project/#what-next","title":"What Next?","text":"<p>Once you have imported your data, you can perform a number of procedures with the other modules. The WE1S project primarily employs topic modeling in its research methodlogy, so this technique is prominent in the Workspace in its current version. Many of the modules depend on your first having run a topic model on your data. This makes the <code>topic_modeling</code> module a good place to start. The <code>metadata</code> module contains some analysis and visualization tools that do not require a pre-existing topic model.</p>"},{"location":"glossary/","title":"Glossary","text":"<ul> <li>\u201cCollection\u201d \u2014 A specific set of of texts (and data about them) that a user is working on inside a project. A collection can be the entirety of a corpus; but it can also be a particular subset of a corpus: e.g., only newspaper articles containing both the words humanities and science.</li> <li>\u201cComputer environment\u201d \u2014 The whole computing platform containing (in replicable containers) the software and tools offered by WE1S. Users can download and install the environment on their own computers.</li> <li>\u201cCorpus / Corpora\u201d \u2014 The total set of texts (and data about them) that a user has collected or is working on. (Compare Collection.)</li> <li>\u201cData\u201d \u2014 Representations of a text collection in derived forms that are not readable as plain text. For example, data that the WE1S Workspace generates from text collections include: bags-of-words or term frequencies, ngram counts, etc.</li> <li>\u201cModule\u201d \u2014 A specific bundle of one or more Jupyter notebooks (and supporting scripts and files) available inside a project folder. Each module focuses on a particular task:  e.g., creating a topic model or visualizing it. Each module contains a <code>README.md</code> file with a user guide for that module.</li> <li>\u201cProject\u201d \u2014 The folder location and file structure created by running the <code>create_new_project.ipynb</code> notebook. A project is where users work on collections of texts and data using the Workspace\u2019s modules.</li> <li>\u201cMetadata\u201d \u2014 Secondary data about the data being worked on in the WE1S Workspace. Metadata includes such citation information about collections of texts as author, publication, date, etc. But it can also include other kinds of labels or tags created by a user to facilitate addressing research questions. For example, the WE1S Project labeled publication sources in some of the collections it topic modeled based on geographical region, kind of publication, self-identified association with particular social groups, etc.</li> <li>\u201cTemplate\u201d \u2014 When a new project is created, folders for each module containing notebooks and supporting scripts or other resources are copied from a central location to your project folder. This is known as the project \u201ctemplate\u201d, and the individual files are called the \u201ctemplate\u201d files. Templates have version numbers so that it is clear what version of the template was used to produce a project even if the template files are updated after the project was created.</li> <li>\u201cWorkspace\u201d \u2014 The Jupyter notebook system within the WE1S computing environment for collecting, managing, analyzing, topic modeling, visualizing, and other operations on texts. When initially downloaded as part of the computing environment, the Workspace includes a Jupyter notebook for initiating a project and installing the necessary modules for managing project workflow.</li> </ul>"},{"location":"how-jupyter-notebooks-work/","title":"How Jupyter Notebooks Work","text":"<p>In the WE1S Workspace, users interact with their data using a Jupyter notebooks-based environment. Jupyter notebooks are a web-based interface for running code inside your web browser. There are two varieties of Jupyter notebooks: the classic Jupyter notebooks interface and the newer Jupyter Lab interface. The WE1S Workspace works with both, but the Jupyter Lab interface is opened by default.</p> <p>A \"notebook\" is like a web page with form fields (called \"cells\") into which you type your Python code. You can then run the code and get feedback. There are a number different notebook implementations, but Jupyter notebooks (formerly called iPython notebooks) are by far the most popular.</p> <p>When you open a specific notebook, you enter (or paste) your code in the cells and then click the <code>Run</code> button or type Shift+Enter to run your code. The WE1S Workspace provides notebooks in which the cells are pre-populated with the code necessary to perform specific tasks. Jupyter notebooks also contain documentation cells (written in a combination of Markdown and HTML) that describe the purpose of the cell's code. In some cases, you will be asked to enter configuration information, and this will require you to write some very basic Python code. If you are new to Python, this section provides instructions that will help you configure your cells. If you are more experienced with Python, you can modify the code in the WE1S Workspace to suit your needs. This provides a very flexible research environment suitable for beginners and more advanced users.</p> <p>Here is a small sample. The cell below contains some code that displays a message, waits 10 seconds, and then displays another message.</p> Sample Jupyter notebook cell. <p>Notice the small <code>In []:</code> to the left of the cell. This tells you that the cell contains some input code. When you run the cell, it will display <code>In [*]:</code>. When the code has finished running, it will display <code>In [1]:</code> The next cell will display <code>In [2]:</code>, and so on. When the code finishes, the output of whatever the code is doing will display right below the cell.  Also notice that there is a small white circle next to the words \"Python 3\" at the top right of the screen. When you are running a cell, the circle will turn grey. It will turn white again when the circle is finished.</p> <p>If you make a mistake when running a cell \u2014 or if there is a bug in the code we haven't caught or an issue we haven't anticipated \u2014 an error message will be displayed in the output section. Note that for longer code blocks, you may have to scroll to see the error, so, if something doesn't seem to be working, look for an error. The meaning of Python error messages can be somewhat opaque until you get used to them, so we recommend copying the error you get, pasting it into a Google search bar, and starting there.</p> <p>You can clear the output by selecting Cell &gt; Current Outputs &gt; Clear or `Cell &gt; All Output &gt; Clear in the menu at the top of the notebook. There are lots of other menu items which allow you to add and delete cells, as well as to run cells.</p> <p>For a full tutorial on Jupyter notebooks, see Quinn Dombrowski, Tassie Gniady, and David Kloster's Introduction to Jupyter Notebooks.</p>"},{"location":"how-jupyter-notebooks-work/#using-jupyter-lab","title":"Using Jupyter Lab","text":"<p>In the Classic notebook environment, you typically start in a browser tab showing your file and folder hierarchy. By clicking the <code>New</code> button at the top right corner, you can open new tabs containing notebooks, text files, or terminals for running processes from the command line. By contrast, Jupyter Lab operates in a single browser tab with the file/folder hierarchy in a sidebar and subtabs containing notebooks, files, and terminals. To open a new subtab, you click the appropriate icon for a notebook, text file, or terminal. One of the other main differences is that many functions are now located in the contextual menu opened by right-clicking on the screen. A screenshot of Jupyter Lab illustrates the appearance of the interface.</p> Screenshot of Jupyter Lab. <p>It is always possible to switch the Classic interface from within Jupyter Lab by selecting \"Launch Classic Notebook\" from the <code>Help</code> menu. For more information, see the Jupyter Lab interface.</p> <p>Hint</p> <p>Classic notebooks will always have the path <code>tree</code> in the url in the browser's address bar. By changing <code>tree</code> to <code>lab</code>, and vice versa, you can switch between the two interfaces.</p>"},{"location":"introduction-to-python/","title":"Introduction to Python","text":"<p>Python is an easy-to-learn programming language that is very popular in the digital humanities, in part because it has a large user community that has developed tools for such tasks as natural language processing and machine learning. You should be aware that there are two common versions of the language: Python 2 and Python 3. Although tools developed in Python 2 are still common, it is no longer supported as of January 2020. All of the code in the WE1S is written in Python 3.7. The differences are fairly minor, but, if you need to Google how do something in Python, be sure that you are getting an answer that is compatible with version 3.7.</p>"},{"location":"introduction-to-python/#basic-python-concepts","title":"Basic Python Concepts","text":""},{"location":"introduction-to-python/#file-paths","title":"File Paths","text":"<p>A file path specifies the unique location of a file on a file system. File paths are important to understand when navigating through your file system using your computer's command line and when using a programming language (for more on Mac command lines, we recommend Miriam Posner's Get to know your terminal); for more on Windows command lines, we recommend How to use the Windows command line). There are 2 common kinds of file paths: absolute and relative. An absolute file path points to the location of a file by displaying the full directory tree hierarchy in which path components, separated by a delimiting character (usually a slash, <code>/</code> or <code>\\</code>, depending on your operating system), represent each directory under which the file is stored.</p> <p>For example, on a Mac or Linux machine, the absolute file path of a text file stored in a folder on a user's Desktop looks something like this:</p> <pre><code>/Users/user_name/Desktop/folder_name/file_name.txt\n</code></pre> <p>On a Windows machine, the absolute file path of a text file stored on a user's Desktop looks like this:</p> <pre><code>C:\\Users\\user_name\\Desktop\\folder_name\\file_name.txt\n</code></pre> <p>A relative file path, in comparison, points to the location of a file relative to \"where\" the user is in the file system. So, if you used your command line to navigate to the folder on your Desktop described above and then looked at the text file, the relative file path would simply be:</p> <pre><code>file_name.txt\n</code></pre> <p>If you then used your command line to move back up to your Desktop and looked for the text file, the relative file path would be: <code>folder_name/file_name.txt</code> (Mac or Linux) or  <code>folder_name\\file_name.txt</code> (Windows).</p> <p>Note</p> <p>The WE1S Workspace runs on a Linux machine, so any file path you see or need to enter will be of the Mac/Linux variety using a forward <code>/</code>.</p> <p>Many file paths will be automatically configured for you in the WE1S Workspace, but you will need to enter file paths from time to time. The instructions in each notebook will tell you whether to enter an absolute or relative file path. If you're not used to working with them, file paths can be vexing because entering in the wrong file path will cause errors in otherwise \"working\" code. In general, if you see a <code>FileNotFoundError</code> output to a cell in your notebook, your issue could be related to file path(s) you have configured.</p>"},{"location":"introduction-to-python/#python-packages","title":"Python Packages","text":"<p>In Python, you use \"functions\" to perform operations. For example, the <code>print()</code> function prints whatever message inside the parentheses to the screen. Python has a set, or \"library\", of functions (known as \"the standard library\") like <code>print()</code>, which can be \"called\" out of the box. It is also possible to import libraries of functions, which are called either \"packages\" or \"modules\", into your code. Some Python packages like <code>time</code> have to be imported; they are not available by default in order to save memory. Other Python packages are designed by third-party contributors. These generally have to be installed on the machine running the code and then imported into the notebook. The WE1S Workspace comes with all required packages pre-installed. In the notebooks, a cell (generally at the beginning of the notebook) is pre-configured to import them into the notebook. You will see this where the cell contains lines like <code>import PACKAGE_NAME</code> (where <code>PACKAGE_NAME</code> represents the name of the package) or <code>from PACKAGE_NAME import FUNCTION_NAME</code>. You will not need to modify this setup, but it is useful to be familiar with the terminology and aware of the purpose of this code.</p>"},{"location":"introduction-to-python/#data-types","title":"Data Types","text":"<p>In most programming languages, content is categorised into types of data, each of which has its own set of behaviours. In particular, some functions can only operate with certain data types. In Python, some of the most common types of content are integers, floats, strings, lists, dictionaries (also known as \"dicts\"), and Booleans (True or False). The data type of any piece of data is defined using delimiter symbols:</p> <ul> <li>strings: enclosed in apostrophes or quotation marks (e.g. <code>'John'</code> or <code>\"John\"</code>).</li> <li>integers and floats: no delimiters (e.g. <code>100</code>, <code>3.14</code>)</li> <li>lists: enclosed in square brackets (e.g. <code>[1, 2]</code> or <code>['Jack', 'Jill']</code>).</li> <li>dicts: enclosed in curly brackets (e.g. <code>{'Jack': 1}</code>).</li> <li>Booleans: <code>True</code> or <code>False</code> (capitalised with no delimiters)</li> </ul> <p>We'll look at each of these datatypes more closely below.</p> <p>Strings contain alphanumeric data \u2014 that is, either numbers or letters, as well as punctuation marks. Strings are delimited either by single or double quotation marks. So <code>'hello'</code> and <code>\"hello\"</code> are exactly the same. In the WE1S Workspace, single quotation marks are used wherever possible (and also because that is a recommendation of a prominent Python code style guide). Quotation marks inside of apostrophe delimiters will be interpreted as part of the string, as will apostrophes inside quotation mark delimiters. For example, you could encode the sentence He said, \"We are ready.\" as <code>'He said, \"We are ready.\"'</code></p> <p>There are a number of gotchas when dealing with strings. First, curly quotes or smart quotes are not interpreted as quotation marks by Python; they will be treated as characters in the string. So be warned. If you try to copy code from a document with curly quotes, you are likely to get an error.</p> <p>Some strings may contain apostrophes or quotation marks that are part of the string, not delimiter symbols. Take, for example, the sentence <code>We're ready.</code>. Coding this as <code>'We're ready.'</code> will generate an error because Python will interpret the second apostrophe as a delimiter, ending the string. An obvious way around this problem is to use double quotation marks as the delimiters: <code>\"We're ready.\"</code>. That works. But what if you wanted to use the sentence <code>He said, \"We're ready.\"</code>? For this sentence you will probably want to use a method called \"escaping\" the ambiguous punctuation mark. In Python, characters like this are normally escaped with a preceding backslash. Here is any easy way to fix the problem: <code>'He said, \"We\\'re ready.\"'</code>. The backslash before the apostrophe tells Python not to interpret it as a string delimiter.</p> <p>Note</p> <p>In the WE1S Workspace we try to use single quotation marks as consistently as possible, switching to double quotation marks only if there is a reason to switch. This conforms with Python's PEP 8 recommendations for code style.</p> <p>Various types of numbers are recognised by Python. The most important types are floats, which can have decimal points, and integers, which cannot. The standard library contains functions to convert one to the other. Their different behaviours mostly become important when you are performing mathematical operations.</p> <p>Mathematical operators are symbols like <code>+</code> and <code>-</code> that tell Python what operations to perform. As you can imagine, they indicate \"add\" and \"subtract\" respectively. Other mathematical operators are <code>*</code> (multiply) and <code>/</code> (divide). It is not necessary to put spaces around the operators, but it is generally good coding style.</p> <p>For a fuller (but still digestible) description of mathematical operators, see Basic Math Operators in Python.</p> <p>Lists are comma-separated items belonging to other data types such as numbers or strings, as in the examples above. You can also have lists of dictionaries, or even lists of lists. Items in lists are stored in a strict order, and each item has an index number. Like most programming languages, Python uses zero-indexing, so the first item in the list will have the index number 0. You can reference items in a list by giving its number in square brackets. For instance, if you had a list called <code>my_list</code>, you could get the first item in the list with <code>my_list[0]</code>.</p> <p>Dictionaries (also called dicts) are like lists, except that each item is a key-value pair (separated by a colon). A common use of dictionaries is to contain word counts. Consider this dictionary: <code>{'Jack': 1, 'Jill': 2}</code>. This might be used to indicate that the word \"Jack\" occurs once and the word \"Jill\" occurs twice.</p> <p>Items in dictionaries are not stored in order, so they must be referenced by their keys. In order to illustrate this, we need to save our dict to a variable a concept that will be explained in full below. We'll call our variable <code>word_counts</code>. We can then find the number of times \"Jill\" occurs by calling <code>word_counts['Jill']</code>. Try running the cell below to demonstrate this. Then experiment with creating your own dicts and re-run the cell.</p> <p>Booleans are binary values, either <code>True</code> or <code>False</code>.</p> <p>Important</p> <p>The Boolean values <code>True</code> and <code>False</code> must be capitalized, and they must not be contained in quotation marks (i.e. not <code>'True'</code> or <code>'False'</code>).</p>"},{"location":"introduction-to-python/#variables","title":"Variables","text":"<p>A variable is a container for some data of any type. This container is referenced by a label or variable name. The content, or value, of a variable is assigned to the variable name using the <code>=</code> sign. For instance, <code>firstname = 'John'</code> would assign the string <code>'John'</code> to the variable <code>firstname</code>. If you also assigned <code>lastname = 'Smith'</code>, then <code>firstname + ' ' + lastname</code> would produce the string <code>'John Smith'</code>. In some cases, you have to create the variable before you assign a value to it. For instance, you might create an empty list like <code>my_list = []</code>.</p> <p>Once you have assigned a value to a variable name, you can reference it by the variable name in your subsequent code. This is what we did with the <code>word_counts</code> variable in the discussion of dictionaries above.</p> <p>You can name variables whatever you want, although they cannot have spaces or punctuation marks. It is good coding style to make variable names short and descriptive of the values they are storing. As in the case of <code>word_counts</code>, it is conventional to use the underscore <code>_</code> where you might use a space in ordinary English. We have tried to follow these guidelines as much as possible in the development of configuration variables for the WE1S Workspace.</p>"},{"location":"introduction-to-python/#functions","title":"Functions","text":"<p>A function is a command to do something with one of the data types and send back (or \"return\") a result. When functions form part of larger libraries, they are often called \"methods\". Since <code>print()</code> and <code>str()</code> are part of the standard library, they are often referred to as the <code>print()</code> and <code>str()</code> methods.</p> <p>Functions are called by using the name of the function (e.g. <code>print</code>) followed by parentheses containing the data to be passed to the function. So <code>print('Hello world.')</code> passes \"Hello world.\" to the <code>print()</code> function.  The value in parentheses is called the argument or parameter. You can use a variable as the argument of a function. For instance, if you defined <code>my_var = 1</code>, you could then use <code>str(my_var)</code> to convert <code>1</code> to a <code>'1'</code>. Another example is the <code>append()</code> function, which adds an item to the end of a list. For instance, you could add the number 1 to the <code>my_list</code> created above: <code>my_list.append(1)</code>.</p> <p>Code that is part of a function is indented by four spaces (a tab also works within the notebook environment). This also applies to certain function-like operations such as <code>for</code> loops or <code>if...then</code> clauses. If you get an error around a function or either of these key words in the code, you may have accidentally changed the indentation.</p> <p>In the WE1S Workspace, many functions are pre-written and called from a single line of code that passes your configuration variables to the function. Complex functions are generally stored in separate files (typically in a <code>scripts</code> folder) and imported into the notebook in order to reduce clutter. Experienced Python users can open and modify the functions files.</p>"},{"location":"introduction-to-python/#comments","title":"Comments","text":"<p>Comments are blocks of text in the code that do not get executed as code. They exist to provide explanations for human readers to understand what the code is doing. In Python, comments are indicated by a preceding <code>#</code>. Anything with this preceding hash will be ignored. In the WE1S Workspace, code is heavily commented to help you see what its purpose is.</p> <p>A common trick is to \"comment out\" blocks of code so that they will not function by placing <code>#</code> at the beginning of the line. The code can then be \"uncommented\" by removing the <code>#</code>. A shortcut for commenting out and uncommenting code in the Jupyter notebooks environment is the key combination Ctrl + <code>/</code>. Use this for toggling commented code on and off.</p>"},{"location":"introduction-to-python/#json","title":"JSON","text":"<p>In the WE1S Workspace, data is typically accessed in a format called JSON. JSON is an acronym for Javascript Object Notation, and, as the name suggests, it derives originally from the Javascript programming language. JSON looks and behaves pretty much like a Python dict with one major gotcha: keys and values have to be in double quotation marks. (There are a few other differences as well, but this is the most obvious). So, if we want to work with JSON data in Python, we need to parse it into a Python dict. Likewise, if we want to, for instance, send the material in a Python dict to a web brower or save it to a file, we first convert it to JSON format, the data type of which is a string. Python has a <code>json</code> library that performs this task, and the WE1S Workspace uses it internally when it accesses project data stored as JSON files.</p>"},{"location":"workspace-structure/","title":"Workspace Structure","text":"<p>The WE1S Workspace is a cloud of virtual computers with all the resources you will require to manage your workflow. Most of your work will be on the <code>write</code> volume, which we will generally refer to as the Workspace. Technically, however, the Workspace also includes other volumes, such as the MongoDB instance described below. Each volume has access to the same data files.</p> <p>This section provides a brief overview of what you will see when you launch the Workspace. An outline of the over structure is displayed below, followed by explanations of each component.</p> <p> <code>write</code> \u2523  <code>getting_started</code> \u2523  <code>project_template</code> \u2523  <code>create_template_archive.ipynb</code> \u2523  <code>getting_started.html</code> \u2523  <code>mongo_express.html</code> \u2523  <code>new_project.ipynb</code> \u2523  <code>template_package.py</code> \u2517  <code>README.md</code></p>"},{"location":"workspace-structure/#the-write-folder","title":"The <code>write</code> Folder","text":"<p>The <code>write</code> folder is the location where you will do most of your work. When you launch the Workspace, you will see five files and two folders located inside your <code>write</code> folder. The functions of each of these is described below.</p>"},{"location":"workspace-structure/#getting_startedhtml","title":"<code>getting_started.html</code>","text":"<p>This is simply a convenient file that you can open to launch this Getting Started guide. It actually just redirects the browser to <code>getting_started/index.html</code>. It is a read-only file and cannot be modified.</p>"},{"location":"workspace-structure/#readmemd","title":"<code>README.md</code>","text":"<p>This is a Markdown file containing information about the version of the project template in your Workspace. If you download a later version, it may no longer be compatible, and you may have to install the most recent version of the Workspace. The information in this file can therefore be useful to check whether your project template and your Workspace container are compatible. The <code>README.md</code> file is a read-only file and cannot be edited.</p>"},{"location":"workspace-structure/#template_packagepy","title":"<code>template_package.py</code>","text":"<p>This is Python script that contains functions used by both the <code>create_template_archive</code> and <code>new_project</code> notebooks. It is a read-only file and cannot be edited.</p>"},{"location":"workspace-structure/#create_template_archiveipynb","title":"<code>create_template_archive.ipynb</code>","text":"<p>This notebook can be used to make a compressed <code>.tar.gz</code> file of your <code>project_template</code> folder. Generally, you will only want to do this if you have customized the template and wish to use your customizations in another project. Use this notebook to create an archive of your custom template, and you can point to it as your template source when creating projects with the <code>new_project</code> notebook.</p>"},{"location":"workspace-structure/#new_projectipynb","title":"<code>new_project.ipynb</code>","text":"<p>This notebook is the main starting point for you workflow. Use the <code>new_project</code> notebook to create a new copy of the <code>project_template</code> folder with metadata about your specific project. You can then enter the new project folder to import your data into the new project.</p>"},{"location":"workspace-structure/#getting_started","title":"<code>getting_started</code>","text":"<p>This folder contains the Getting Started web site you are currently reading. The site can be launched by going to <code>getting_stared/index.html</code> or by launching <code>getting_started.html</code> from the Workspace <code>write</code> folder. You should not need to modify anything in this folder, unless you want to use the Getting Started site as a place to add notes to yourself in the site's individual web pages.</p>"},{"location":"workspace-structure/#project_template","title":"<code>project_template</code>","text":"<p>This folder contains are the files and resources required for the individual modules in WE1S Workspace. When a new project is created, these resources are copied from the <code>project_template</code> folder into the new project folder and become available for use in the new project.</p>"},{"location":"workspace-structure/#mongodb","title":"MongoDB","text":"<p>The Workspace comes packaged with an instance of the MongoDB database, running on <code>mongodb://mongo:27017</code>. You can use this the MongoDB instance to manage your data. MongoDB stores data in json-like records similar to the json files used in Workspace projects, making the interchange of data between the two relatively transparent. Modules access MongoDB databases through Python's pymongo wrapper. The use of MongoDB is optional. You can also choose to import your data from flat files.</p> <p>If you wish to use MongoDB, the <code>mongo-express.html</code> file will launch MongoExpress, a web-based admin interface for MongoDB, running on port 8081. This will allow you to perform some database administration tasks without running Python code.</p> <p>To Do</p> <p>What other information might be useful here?</p>"},{"location":"modules/","title":"Modules","text":"<p>This section provides brief overviews of the individual modules in the Workspace. More information and a User Guide for each module can be found on the individual module pages and in the <code>README.md</code> files inside each module folder.</p>"},{"location":"modules/#comparing","title":"<code>comparing</code>","text":"<p>The <code>comparing</code> module allows you to compare two sets of textual data to one another to discover how, and how much, they differ using the Wilcoxon rank sum test, a statistical test that determines if the relative frequencies of specific words in two populations are significantly different from one another (that is, they have different distributions). This helps you to determine what the most \"significant\" words in each dataset are.</p> <p>For more on the Wilcoxon rank sum test in the context of corpus analytics, see Jefrey Lijffijt, Terttu Nevalainen, Tanja S\u00e4ily, Panagiotis Papapetrou, Kai Puolam\u00e4ki, Heikki Mannila, \"Significance testing of word frequencies in corpora\", Digital Scholarship in the Humanities, Volume 31, Issue 2, June 2016, Pages 374\u2013397.</p>"},{"location":"modules/#counting","title":"<code>counting</code>","text":"<p>The <code>counting</code> module contains notebooks for counting various aspects of project data. The notebooks allow you to count project documents (<code>count_documents.ipynb</code>), to count the number of documents containing a specific token (<code>docs_by_search_term.ipynb</code>), to calculate token frequencies (<code>frequency.ipynb</code>), to calculate tf-idf scores (<code>tfidf.ipynb</code>), to calculate various collocation metrics (<code>collocation.ipynb</code>), and to grab summary statistics of documents, tokens, etc in the project (<code>vocab.ipynb</code>). The <code>vocab.ipynb</code> notebook requires that your json data files contain a <code>bag_of_words</code> field with term counts. If you did not generate this field when you imported your data, you can do so using <code>tokenize.ipynb</code>, which leverages spaCy's tokenizer. The <code>docs_by_search_term.ipynb</code>, <code>frequency.ipynb</code>, <code>tfidf.ipynb</code>, and <code>collocation.ipynb</code> notebooks use a custom tokenizer based on the tokenizer available in NLTK. This differs from the tokenizer WE1S uses in its preprocessing and topic modeling pipelines, which only tokenizes unigrams. As a result, some features of these notebooks will not work if you do not have access to full-text data.</p>"},{"location":"modules/#dendrogram","title":"<code>dendrogram</code>","text":"<p>The <code>dendrogram</code> module performs hierarchical agglomerative clustering on the topics of a topic model or multiple topic models in a project. Several distance metrics and linkage methods are available. The default settings will conform closely to the output of pyLDAvis and the scaled view of dfr-browser. Information on alternative distance metrics can be found here, and information on alternative linkage methods can be found here, in the documentation for the Python <code>scipy</code> package, which is used \"under the hood\".</p> <p>As dendrograms typically become crowded and hard to read, the visualizations are generated with Plotly, which allows them to be browsed interactively with its pan and zoom features.</p>"},{"location":"modules/#dfr_browser","title":"<code>dfr_browser</code>","text":"<p>The <code>dfr_browser</code> module implements the creation and customization of Andrew Goldstone's dfr-browser from a topic model produced with MALLET (the output of the <code>topic_modeling</code> module). Dfr-browser code, stored in this module in <code>dfrb_scripts</code>, was written by Andrew Goldstone and adapted for WE1S use to provide an easy pipeline from data import to topic modeling to visualisation. WE1S uses an older version of Goldstone's code (v0.5.1) (see https://agoldst.github.io/dfr-browser/ for a version history). WE1S uses Goldstone's <code>prepare_data.py</code> Python script to prepare the data files, not Goldstone's R package.</p>"},{"location":"modules/#diagnostics","title":"<code>diagnostics</code>","text":"<p>The <code>diagnostics</code> module produces a modified version of the diagnostics visualization on the MALLET website. This includes various metrics for analyzing the quality of topic models. Users can generate diagnostics to visualize a single model or a comparative visualisation for multiple models.</p>"},{"location":"modules/#export","title":"<code>export</code>","text":"<p>The <code>export</code> module provides utilities for exporting data from a project or a project as a whole to a compressed <code>tar.gz</code> archive. The <code>tar</code> format is preferred to the <code>zip</code> format for entire projects because it preserves file permissions. This is less important for exports of the data itself.</p>"},{"location":"modules/#import","title":"<code>import</code>","text":"<p>The <code>import</code> module is main starting point for using the WE1S Workspace project. Use this module to import data into your project. Imported data is stored in the <code>project_data</code> folder. The import pipeline attempts to massage all data into a collection of JSON files, stored in the <code>project_data/json</code> folder. These JSON files contain both data and metadata as a series of key-value pairs that conform to the JSON file format. The names of the keys (also called \"fields\") conform to the WE1S manifest schema and certain metadata required by the tools in the WE1S Workspace. After import, the text content of your data will be stored in the JSON files' <code>content</code> field. JSON files are human-readable text files, which you can open and inspect. However, it is very easy to corrupt their format. If you ever suspect that this may have happened, you can paste the text into a JSON validator like jsonlint.com to check for errors.</p> <p>The import notebook requires either a zip archive of your data or a connection to a MongoDB database where the data is stored. Zip archives may take one of the following forms:</p> <ol> <li>A zip archive containing plain text data and an accompanying CSV file with relevant metadata.</li> <li>A zip archive containing data already in JSON format.</li> <li>A zip archive of a Frictionless Data data package containing data already in JSON format.</li> </ol> <p>If your metadata does not contain the field names required by the WE1S workspace, the import module allows you to map your metadata's field names onto the WE1S field names that comply with the WE1S manifest schema. Most modules in the Workspace assume that your project's json files contain schema-compliant fields.</p>"},{"location":"modules/#json_utilities","title":"<code>json_utilities</code>","text":"<p>This module provides helpful methods of accessing the contents of you project's <code>json</code> folder. This folder can be quite large, and may cause the browser to freeze if opened using the Jupyter notebook file browser. The <code>json_utilities.ipynb</code> notebook in this module provides methods for reading the contents of files in the <code>json</code> folder and for performing database-like queries on its contents to filter results.</p> <p>The <code>remove_fields.ipynb</code> notebook will remove specified field from all files in the <code>json</code> folder. It is intended primarily for creating sample data sets for testing or for removing content that might be subject to intellectual property restrictions.</p>"},{"location":"modules/#metadata","title":"<code>metadata</code>","text":"<p>The <code>metadata</code> module enables you to generate some basic statistics about document counts based on the metadata fields available in your project's json files. You can also use the `add_metada1 notebook to add custom metadata fields to your project's json files after the data has been imported.</p> <p>This module also contains a notebook for generating visualizations based on project data using the Scattertext library.</p>"},{"location":"modules/#pyldavis","title":"<code>pyldavis</code>","text":"<p>This module allows you to produce pyLDAvis visualizations of topic models produced using MALLET (the output of the <code>topic_modeling</code> module). pyLDAvis is a port of the R LDAvis package for interactive topic model visualization by Carson Sievert and Kenny Shirley.</p> <p>pyLDAvis is designed to help users interpret the topics in a topic model by examining the relevance and salience of terms in topics. Once a pyLDAvis object has been generated, many of its properties can be inspected as in tabular form as a way to examine the model. However, the main output is a visualization of the relevance and salience of key terms to the topics.</p> <p>pyLDAvis is not designed to use MALLET data out of the box. This notebook transforms the MALLET state file into the appropriate data formats before generating the visualization. The code is based on Jeri Wieringa's blog post Using pyLDAvis with Mallet and has been slightly altered and commented.</p>"},{"location":"modules/#topic_bubbles","title":"<code>topic_bubbles</code>","text":"<p>This module creates a topic bubbles visualization from dfr-browser data generated in the dfr-browser notebook or from model data generated in the <code>topic_modeling</code> module. This module uses scripts originally written by Sihwa Park for the WE1S project. For more information on Park's script, see Park's topic bubbles Github repo and the <code>README.md</code> located in this module's <code>tb_scripts</code> folder.</p>"},{"location":"modules/#topic_modeling","title":"<code>topic_modeling</code>","text":"<p>This module uses MALLET to topic model project data. Prior to modeling, the notebooks extract word vectors from the project's json files into a single <code>doc_terms.txt</code> file, which is then imported into MALLET. Stop words are stripped at this stage. By default, the WE1S Standard Stoplist contained in the <code>scripts</code> folder is used for this process. After modeling is complete, there is an option to create a scaled data file for use in visualisations such as <code>dfr-browser</code> and <code>pyLDAvis</code>.</p>"},{"location":"modules/#utilities","title":"<code>utilities</code>","text":"<p>This module is really a container for miscellaneous \"helper\" notebooks that can be used to interact with the data in the project. <code>clear_caches.ipynb</code> provides various methods for deleting data, clearing notebook outputs, or resetting a project folder to its original state.</p> <p><code>zip_folder.ipynb</code> provides code to save any folder as a zip archive so that it can be exported from the Workspace.</p>"},{"location":"modules/comparing/","title":"Comparing","text":""},{"location":"modules/comparing/#about-this-module","title":"About This Module","text":"<p>This module allows you to compare two sets of textual data to one another to discover how, and how much, they differ based on word frequency. The notebook performs this comparison using the Wilcoxon rank sum test, a statistical test that determines if two samples (i.e., the relative frequencies of a specific word in two different datasets) are taken from populations that are significantly different from one another (meaning, if they have different distributions). When used to compare word frequency data from two different datasets, it helps you to determine what the most \"significant\" words in each dataset are.</p> <p>For details on the Wilcoxon rank sum test, see this description by the University of Virgina Library. For implementations of the Wilcoxon rank sum test in literary studies, see Andrew Piper and Eva Portelace's article How Cultural Capital Works: Prizewinning Novels, Bestsellers, and the Time of Reading (Post45, 5.10.16), and chapter 4 \"Fictionality\" from Andrew Piper's book Enumerations.</p>"},{"location":"modules/comparing/#user-guide","title":"User Guide","text":"<p>This module has only one notebook: <code>compare-word-frequencies.ipynb</code>. Prior to beginning the workflow, you already must have two <code>doc-terms</code> files, one for each dataset you wish to compare. A <code>doc-terms</code> file is a text file representing the vocabulary in each document in the dataset. <code>doc-terms</code> files must contain rows of space-delimited columns beginning with the filename and index number (starting with 0), and then a list of every token in the document (with each instance of a token in its own column). Each row in each file should represent one document. A small example is given below:</p> <pre><code>sample_document1.json 0 21st a a a an an absolutely abyss academic addition advance afghanistan...\nsample_document2.json 1 a a a an an an artificial artificial food food ingredients...\n</code></pre> <p>As you can see in the example above, the data in these files should already be tokenized and (ideally) stripped of stop words. If you have already prepared your data for topic modeling, there will be a <code>doc_terms.txt</code> file in your project's <code>project_data/models</code>, which you can use as an example of the format. This notebook does not include any processes for tokenizing or stripping stop words from document data.</p> <p>Important</p> <p>The two doc-terms files you are comparing must not have overlapping filenames.</p> <p>The <code>compare_word_frequencies.ipynb</code> notebook provides 3 different methods to help users select data for this test: you can simply provide the filepaths to the <code>doc-terms</code> files you want to use; you can use a list of filenames to select only specific files from a given <code>doc-terms</code> file; and/or you can select a random sampling of files from a given <code>doc-terms</code> file. Each of these options is described in the notebook.</p> <p>Once you have your <code>doc-terms</code> files ready, you can prepare your data for the test and then run the test. Please refer to the notebook for additional instructions about how to run the test.</p>"},{"location":"modules/comparing/#module-structure","title":"Module Structure","text":"<p> comparing  \u2523  results  \u2523  scripts  \u2503 \u2523  compare_word_frequencies.py  \u2523   compare-word-frequencies.ipynb  \u2517  README.md</p>"},{"location":"modules/counting/","title":"Counting","text":""},{"location":"modules/counting/#about-this-module","title":"About This Module","text":"<p>This module contains notebooks for counting various aspects of project data. The notebooks allow you to count project documents (<code>count_documents.ipynb</code>), to count the number of documents containing a specific token (<code>docs_by_search_term.ipynb</code>), to calculate token frequencies (<code>frequency.ipynb</code>), to calculate tf-idf scores (<code>tfidf.ipynb</code>), to calculate various collocation metrics (<code>collocation.ipynb</code>), and to grab summary statistics of documents, tokens, etc in the project (<code>vocab.ipynb</code>). The <code>vocab.ipynb</code> notebook requires that your json data files contain a <code>bag_of_words</code> field with term counts. If you did not generate this field when you imported your data, you can do so using <code>tokenize.ipynb</code>, which leverages spaCy's tokenizer. The <code>docs_by_search_term.ipynb</code>, <code>frequency.ipynb</code>, <code>tfidf.ipynb</code>, and <code>collocation.ipynb</code> notebooks use a custom tokenizer based on the tokenizer available in NLTK. This differs from the tokenizer WE1S uses in its preprocessing and topic modeling pipelines, which only tokenizes unigrams. As a result, some features of these notebooks will not work if you do not have access to full-text data.</p> <p>Notebooks in this module allow users to configure their text input field -- in other words, you can tell the code where to look to find the text you want to process. You have three options for this: the <code>content</code> field, the <code>bag_of_words</code> field, or the <code>features</code> field. The code expects data in these fields to be in the following formats:</p> <ul> <li><code>content</code>: Full, plain-text data, stored as a string in each document.</li> <li><code>bag_of_words</code>: A bag of words dictionary, where each key is a unique unigram, and each value is a count of the number of times that token appears in the document. The <code>import</code> module allows users to create the <code>bag_of_words</code> field and add it to their project data. Data is alphabetized by default, meaning the bags are not reconstructable.</li> <li><code>features</code>: This field is inserted by the WE1S preprocessor using spaCy, and the recommended <code>content_field</code> to use if working with WE1S public data. It is a list of lists that contains information of the following kinds about each token in the document: <code>[\"TOKEN\", \"NORM\", \"LEMMA\", \"POS\", \"TAG\", \"STOPWORD\", \"ENTITIES\"]</code>. NORM is a lowercased version of the token. LEMMA is the dictionary headword (so the lemma of \"going\" is \"go\"). POS is the part of speech according to spaCy's taxonomy. TAG is the equivalent in the Penn-Treebank system. ENTITIES is a named entity as classified here. Lemmas, POS, tags, and entities are all predicted by spaCy using its language model. STOPWORD is whether or not the lower case form of a token is classed as a stop word in a stoplist. For WE1S data, this is the WE1S Standard Stoplist. spaCy has its own stoplist, and users can also supply their own. Alphabetized by default.</li> </ul> <p>If your json documents do not have a <code>content</code> field (if you are using publicly released WE1S data, for instance), and you are using the <code>bag_of_words</code> or <code>features</code> field as your text input, you will not be able to use some of the functions available in some notebooks in this module. You will also only be able to count unigrams (since all word bags or features tables are alphabetized and thus bigrams and trigrams are not reconstructable).</p>"},{"location":"modules/counting/#user-guide","title":"User Guide","text":"<p>What follows are brief summaries of each notebook in this module. The notebooks themselves are flexible and have a wide range of functionality. For this reason, they are heavily documented and provide information about how to use them and what their different sections mean. Please refer to the notebooks for instructions about how to use the notebooks.</p>"},{"location":"modules/counting/#docs_by_search_termipynb","title":"<code>docs_by_search_term.ipynb</code>","text":"<p>This notebook allows you to count the number of documents in a project containing a specific word or phrase. You can also save document metadata to a dataframe, which you can explore in the notebook or download to your own machine. This notebook also allows you to download the documents containing this word or phrase themselves as either json or txt files.</p>"},{"location":"modules/counting/#frequencyipynb","title":"<code>frequency.ipynb</code>","text":"<p>This notebook provides methods for calculating raw and/or relative frequency values for ngrams (uni-, bi-, and/or trigrams are accepted) within a single document or across all of the project's documents.</p>"},{"location":"modules/counting/#tfidfipynb","title":"<code>tfidf.ipynb</code>","text":"<p>Tf-idf, or term frequency - inverse document frequency, is a common way of measuring the importance of tokens both within a given document and across your project as a whole. You calculate a token's tf-idf score by multipling its relative frequency within a given document by the inverse of the number of documents that token appears in throughout the corpus. See TF-IDF from scratch in python on real world dataset for a more in-depth explanation of the math.</p> <p>Generally speaking, tokens with higher tf-idf scores (those closer to 1) are more important to a given document or corpus. At the document level, \"distinctive\" is a rough synonym for \"important;\" tf-idf provides a way to discover the tokens that are most distinctive within each document in your project. At the corpus or project level, a higher average tf-idf score means that a token is more frequently a distinctive word for documents within your corpus, i.e., it is potentially an important token for understanding your corpus overall.</p>"},{"location":"modules/counting/#collocationipynb","title":"<code>collocation.ipynb</code>","text":"<p>Collocation is another way of discussing co-occurrence; in natural language processing, the term \"collocation\" usually refers to phrases of two or more tokens that commonly occur together in a given context. You can use this notebook to understand how common certain bi- and trigrams are in your project. Generally speaking, the more tokens you have in your project, and the larger your project data is, the more meaningful these metrics will be.</p> <p>This notebook allows you to calculate five different collocation metrics:  1) Likelihood ratio; 2) Mutual information (MI) scores; 2) Pointwise mutual information (PMI) scores; 4) Student's t-test; and 5) Chi-squared test. See below for more information on each metric.</p> <p>Collocation metrics are only useful when you can tokenize on bi- and trigrams. Therefore, this notebook assumes your documents include full-text data, and that this data is stored as a string in the <code>content</code> field of each document.</p>"},{"location":"modules/counting/#likelihood-ratio","title":"Likelihood Ratio","text":"<p>Likelihood ratios reflect the likelihood, within a given corpus (i.e., all documents in the project), of a specific bi- or trigram occurring (technically it tells us the likelihood that any two or three given words exist in a dependent relationship). The higher a likelihood score, the more strongly associated the words composing the bi- or trigram are with one another, roughly speaking. Likelihood ratios usually perform better than t-tests or chi-squared tests (see below) on sparse data (i.e., bigrams and trigrams), and so are often used in natural language processing. The code below is an implementation of Dunning's log likelihood test.</p> <p>For more on likelihood ratios in natural language processing, see Foundations of Statistical Natural Language Processing, pages 161-164, and How to Interpret Python NLTK Bigram-Likelihood Ratios.</p>"},{"location":"modules/counting/#mutual-information-mi-score","title":"Mutual Information (MI) Score","text":"<p>The MI score is a measure of the strength of association between any given token in a project and all of the project's tokens. An MI score measures how much more likely the words of a bi- or trigram are to co-occur within your project than they are to occur by themselves. A higher score indicates a higher strength of association. For more on this concept, see this guide on the mutual information scores and t-tests (see below) in corpus linguistics.</p> <p>The code in this notebook implements NLTK's version of mutual information. See NLTK documentation here.</p> <p>MI scores are sensitive to unique words, which can make results less meaningful because often unique words will occur much less frequently throughout a corpus. Therefore, we recommend employing a frequency filter when calculating MI scores; the notebook includes this capability.</p>"},{"location":"modules/counting/#pointwise-mutual-information-pmi-score","title":"Pointwise Mutual Information (PMI) Score","text":"<p>PMI scores build on MI scores. Like MI scores, PMI scores measure the association between any given token in a project and all of the project's tokens. A PMI score measures how much more likely the words of a bi- or trigram are to co-occur within your project than they are to occur by themselves. A higher score indicates a higher strength of association. It differs from an MI score in that it refers to single comparisons, while MI scores are a measure of the average PMI scores over all comparisons. For more on this concept, see Gerlof Bouma's widely cited paper, Normalized (Pointwise) Mutual Information in Collocation Extraction.</p> <p>Like MI scores, PMI scores are sensitive to unique words, which can make results less meaningful because often unique words will occur much less frequently throughout a corpus. Therefore, we recommend employing a frequency filter when calculating MI scores; the notebook includes this capability.</p>"},{"location":"modules/counting/#students-t-test","title":"Student's T-test","text":"<p>The student's t-test is perhaps one of the most widely used methods of hypothesis testing. This implementation of the t-test assumes that the words in any bi- or trigram are independent, and it measures how likely the words are to appear together in your project. Like the PMI score, a higher t-test score indicates a higher likelihood that the words in the bi- or trigram occur together in your project than that they occur separately. However, t-tests and chi-square tests (see below) have been shown to not perform as well with sparse data like bigrams and trigrams.</p> <p>You can find a good general discussion of what a t-test is here.</p>"},{"location":"modules/counting/#chi-square-test","title":"Chi-Square Test","text":"<p>The chi-square test is another test of statistical significance. Like a t-test, a chi-squared test assumes that the words in any bi- or trigram are independent. But unlike a t-test, a chi-squared test does not assume a normal distribution. The code below uses an implementation of Pearson's chi-square test of association. As with PMI and t-test scores, a higher chi-squared test score indicates a greater degree of likelihood, i.e., a higher likelihood that the words in a given bi- or trigram occur together in your project.</p> <p>Decent explanations of the chi-squared test can be found here and here.</p>"},{"location":"modules/counting/#tokenizeipynb","title":"<code>tokenize.ipynb</code>","text":"<p>Normally text analysis tools have to divide a text into countable \"tokens\" (most frequently words). This process is called tokenization. This cell allows you to pre-tokenize your data so that other tools do not need to take this step. It generates a dictionary of token-count pairs such as <code>{\"cat\": 3, \"dog\": 2}</code> for each of your JSON files. This dictionary is appended to the JSON file in the <code>bag_of_words</code> field.</p> <p>This notebook offers two tokenization methods. The default method is strips all non-alphanumeric characters and then divides the text into tokens on white space. Alternatively, you can use the spaCy Natural Language Processing library to tokenize based on spaCy's language model. spaCy extracts linguistic <code>features</code> from your text, not only tokens but parts of speech and named entities. This is instrinsically slower and may require a lot of memory for large texts. To use WE1S's custom spaCy tokenizer, set <code>method='we1s'</code>. If your text has been previously processed by spaCy and there is a <code>features</code> table in your JSON file, the tokenizer will attempt to use it to build the <code>bag_of_words</code> dictionary.</p> <p>Errors will be logged to the path you set for the log_file.</p>"},{"location":"modules/counting/#vocabipynb","title":"<code>vocab.ipynb</code>","text":"<p>This notebook allows you to build a single json vocab file containing term counts for all the documents in your project's json directory. It also allows you to access information about the vocab in a convenient manner. If your data does not already have <code>bag_of_words</code> fields, you should run <code>tokenize.ipynb</code> first.</p>"},{"location":"modules/counting/#module-structure","title":"Module Structure","text":"<p> counting \u2523  scripts  \u2503 \u2523  count_docs.py  \u2503 \u2523  count_tokens.py  \u2503 \u2523  tokenizer.py  \u2503 \u2523  vocab.py  \u2523   collocation.ipynb  \u2523   count_documents.ipynb  \u2523   docs_by_search_term.ipynb  \u2523   frequency.ipynb  \u2523   tfidf.ipynb  \u2523  README.md  \u2523   tokenize.ipynb  \u2517   vocab.ipynb</p>"},{"location":"modules/dendrogram/","title":"Dendrogram","text":""},{"location":"modules/dendrogram/#about-this-module","title":"About This Module","text":"<p>This module performs hierarchical agglomerative clustering on the topics of a topic model or multiple topic models in a project. Several distance metrics and linkage methods are available. The default settings will conform closely to the output of pyLDAvis and the scaled view of dfr-browser. Information on alternative distance metrics can be found here and information on alternative linkage methods can be found here.</p> <p>As dendrograms typically become crowded and hard to read, the visualisations are generated with Plotly, so that they can be browsed interactively with its pan and zoom features.</p>"},{"location":"modules/dendrogram/#module-organization","title":"Module Organization","text":"<p>Plotly exports dendrograms as HTML <code>&lt;div&gt;</code> elements which can be inserted into web pages. These are stored in the <code>partials</code> folder as <code>.html</code> files. The <code>dendrogram</code> module functions can then access these elements by inserting them into web pages that load Plotly's Javascript functions off the internet. Because these web pages access the internet, they will only work in a server environment. The <code>dendrogram</code> module also allows you to combine the partials and Plotly Javascript in a single, standalone HTML file that will work without an server environment, but they can be very large (several megabytes).</p> <p>The module consists of two notebooks, <code>create_dendrogram.ipynb</code>, which allows you to create single dendrograms, and <code>batch_dendrogram.ipynb</code>, which allows you to create multiple dendrograms at once. <code>batch_dendrogram.ipynb</code> also enables you to generate an html index file for swapping between visualisations.</p>"},{"location":"modules/dendrogram/#user-guide","title":"User Guide","text":""},{"location":"modules/dendrogram/#create_dendrogramipynb","title":"<code>create_dendrogram.ipynb</code>","text":""},{"location":"modules/dendrogram/#setup","title":"Setup","text":"<p>Imports Python libaries and scripts to the notebook and defines important file paths.</p>"},{"location":"modules/dendrogram/#configuration","title":"Configuration","text":"<p>To select the model you want to produce a dendrogram for: Navigate to the <code>your_project_name/project_data/models</code> directory in your project. Note the name of each subdirectory in that folder. Each subdirectory should be called <code>topicsn1</code>, where <code>n1</code> is the number of topics you chose to model. You should see a subdirectory for each model you produced. To choose which model you would like to produce a dendrogram visualization for, change the value of <code>selection</code> in the cell below to the corresponding subdirectory. For example, if you wanted to produce a dendrogram for the 50-topic model you created, change the value of <code>selection</code> below to this:</p> <p><code>selection = 'topics50'</code></p> <p>Please follow this format exactly.</p> <p>Note</p> <p>You can select only one model to produce a dendrogram for at a time.</p> <p>The dendrogram will be saved to the name you set for the <code>filename</code> configuration. It should end in <code>.html</code>. The file will be saved to the module's <code>partials</code> folder. It is often useful to give files names like <code>topics25-euclidean-single.html</code> to indicate the number of topics, distance metric, and linkage method used for the cluster analysis. This is especially helpful if you want to run the Create Index for Multiple Dendrograms section below.</p> <p>Set the distance metric to <code>euclidean</code> and <code>cosine</code>. The linkage method may be <code>single</code>, <code>complete</code>, <code>average</code>, or <code>ward</code>. Ward linkage requires that the distance metric be set to <code>euclidean</code>.</p> <p>Important</p> <p>The default output will only work in a server environment. If you wish to create a standalone version that can be run on a local computer, set <code>standalone=True</code>. The disadvantage of this method is that the file will be large, about 3 MB.</p>"},{"location":"modules/dendrogram/#load-data-from-the-mallet-state-file","title":"Load Data from the MALLET State File","text":"<p>This cell instantiates a <code>Model</code> object and loads the model's data from the MALLET state file.</p>"},{"location":"modules/dendrogram/#cluster-the-model","title":"Cluster the Model","text":"<p>By default, the cluster wil be saved as an html <code>div</code> element in the <code>partials</code> folder.</p>"},{"location":"modules/dendrogram/#create-web-page-for-a-single-dendrogram","title":"Create Web Page for a Single Dendrogram","text":"<p>This cell generates a web page that displays a single dendrogram. This web page will not work without an internet connection. If you wish to download a copy that does not require an internet connection, set the <code>standalone</code> configuration to <code>True</code> in the Configuration section and re-run the cluster analysis.</p> <p>If you have already produced multiple dendrograms and wish to publish them with an index page, skip to the next section.</p>"},{"location":"modules/dendrogram/#create-index-page-for-multiple-dendrograms","title":"Create Index Page for Multiple Dendrograms","text":"<p>This section will produce an index page allowing you to navigate between multiple dendrogram files, which you have already created. Make sure that you configure the settings as described below.</p> <p>The <code>source_filenames</code> must be the same as dendrogram filenames in your <code>partials</code> folder.</p> <p>The <code>menu_items</code> and <code>dendrogram_titles</code> lists should correspond to the order of the <code>source_filenames</code>. The former will appear as menu labels for navigating between dendrograms, and the latter will be titles for each dendrogram.</p> <p>If you would like to save the index and dendrograms to a zip archive for export, set <code>zip=True</code>.</p> <p>Important</p> <p>The default output will only work in a server environment.</p>"},{"location":"modules/dendrogram/#generate-the-index-page","title":"Generate the Index Page","text":"<p>This cell will display a link to your index page.</p>"},{"location":"modules/dendrogram/#batch_dendrogramipynb","title":"<code>batch_dendrogram.ipynb</code>","text":"<p>This notebook allows you to perform hierarchical cluster analysis on multiple models with multiple clustering options. The output is an HTML index file which allows you to display the generated cluster analyses as dendrograms.</p> <p>The last (optional) cell in this notebook allows you to generate standalone HTML files for a list of already-generated dendrograms.</p>"},{"location":"modules/dendrogram/#setup_1","title":"Setup","text":"<p>Imports Python libaries and scripts to the notebook and defines important file paths.</p>"},{"location":"modules/dendrogram/#configuration_1","title":"Configuration","text":"<p>Provide a list of all models you wish to cluster and the distance metrics and linkage methods you wish to apply to each of the models.</p> <p>Set <code>models = []</code> if you wish to cluster all the models available in our project. Otherwise, provide a list of the folder names for each model you wish to cluster.</p> <p>Available distance metrics are 'euclidean' and 'cosine'.</p> <p>Available linkage methods are 'average', 'single', 'complete', and 'ward'.</p> <p>You will most likely not need to adjust the advanced configuration options, but their use is described below:</p> <p><code>orientation</code> ('top', 'left', 'bottom', 'right'): The location of the dendrogram root <code>height</code>: The height of the dendrogram in pixels <code>width</code>: The width of the dendrogram in pixels <code>hovertext</code>: A list of hovertext for constituent traces of dendrogram clusters <code>truncate_mode</code>:  The dendrogram can be hard to read when the original observation matrix from which the linkage is derived is large. Truncation is used to condense the dendrogram. There are several modes: <code>None</code> (no truncation, the default), <code>lastp</code> (the last <code>p</code> non-singleton clusters formed in the linkage are the only non-leaf nodes in the linkage), 'level' (No more than <code>p</code> levels of the dendrogram tree are displayed). <code>p</code>: The <code>p</code> parameter for <code>truncate_mode</code> <code>color_threshold</code>: The value at which all descendent links below a cluster node will be given the same colour</p> <p>For further details, see the <code>scipy.cluster.hierarchy.dendrogram documentation</code></p>"},{"location":"modules/dendrogram/#cluster","title":"Cluster","text":"<p>This cell starts the cluster analysis and can be run without modification.</p>"},{"location":"modules/dendrogram/#create-standalone-dendrograms","title":"Create Standalone Dendrograms","text":"<p>Run the cells in this section if you wish to create standalone versions of any of the dendrograms you have already created. They will be saved into your project's <code>dendrogram</code> module folder. The dendrograms can be downloaded and will work locally, as long as you have an internet connection. You only need to configure the list of dendrogram names you wish to save.</p>"},{"location":"modules/dendrogram/#module-structure","title":"Module Structure","text":"<p> dendrogram  \u2523  partials  \u2523  scripts  \u2503 \u2523  batch_cluster.py  \u2503 \u2523 index_template.html  \u2503 \u2523  model.py  \u2503 \u2517  standalone.py  \u2523   batch_dendrogram.ipynb  \u2523   create_dendrogram.ipynb  \u2517  README.md</p>"},{"location":"modules/dfr-browser/","title":"Dfr-Browser","text":""},{"location":"modules/dfr-browser/#about-this-module","title":"About This Module","text":"<p>The notebooks in this module implement the creation and customization of Andrew Goldstone's dfr-browser from a topic model produced with MALLET. Dfr-browser code, stored in this module in <code>dfrb_scripts</code>, was written by Andrew Goldstone and adapted for WE1S use and data. WE1S uses an older version of Goldstone's code (v0.5.1); see https://agoldst.github.io/dfr-browser/ for a version history of Goldstone's code. WE1S uses Goldstone's <code>prepare_data.py</code> Python script to prepare the data files, NOT the R package.</p> <p>This module has two notebooks: <code>create_dfrbrowser.ipynb</code> for creating dfr-browsers and <code>customize_dfrbrowser.ipynb</code> for customizing the dfr-browser's display.</p>"},{"location":"modules/dfr-browser/#user-guide","title":"User Guide","text":""},{"location":"modules/dfr-browser/#create_dfrbrowseripynb","title":"<code>create_dfrbrowser.ipynb</code>","text":""},{"location":"modules/dfr-browser/#settings","title":"Settings","text":"<p>The Settings cell defines paths and important variables used to create a Dfr-Browser visualization. The default settings will create a folder inside the dfr_browser module for each topic model in your project or for a selection of models. In most cases, you will not need to change the default settings.</p>"},{"location":"modules/dfr-browser/#create-dfr-browser-metadata-files-from-json-files","title":"Create Dfr-Browser Metadata Files from JSON Files","text":"<p>The <code>dfrb_metadata()</code> function opens up each json in your project's json directory and grabs the metadata information dfr-browser needs. It creates both the <code>metadata_csv_file</code> file and the <code>browser_meta_file_temp</code> file.</p>"},{"location":"modules/dfr-browser/#create-browser-create-files-needed-for-dfr-browser","title":"Create Browser: Create files needed for Dfr-browser","text":"<p>By default, this notebook is set to create Dfr-browsers for all of the models you produced using the <code>topic_modeling</code> module. If you would like to select only certain models to produce Dfr-browsers for, make those selections in the next cell (see next paragraph). Otherwise leave the value in the next cell set to <code>All</code>, which is the default.</p> <p>To produce browsers for a selection of the models you created, but not all: Navigate to the <code>your_project_name/project_data/models</code> directory in your project. Note the name of each subdirectory in that folder. Each subdirectory should be called <code>topicsn1</code>, where <code>n1</code> is the number of topics you chose to model. You should see a subdirectory for each model you produced. To choose which subdirectory/ies you would like to produce browsers for, change the value of selection in the cell below to a list of subdirectory names. For example, if you wanted to produce browsers for only the 50- and 75-topic models you created, change the value of selection below to to <code>selection = ['topics50','topics75']</code>.</p> <p>The <code>get_model_state()</code> function grabs the filepaths of model subdirectories in order to visualize and their state and scaled files. Optionally, you can instead set values for <code>subdir_list</code>, <code>state_file_list</code>, and <code>scaled_file_list</code> manually in the second cell.</p> <p>The <code>create_dfrbrowser()</code> function creates the files needed for Dfr-browser, using the model state and scaled files for all selected models. It prints output from Goldstone's <code>prepare_data.py</code> script to the notebook cell. Once your Dfr-browser(s) have been created, the <code>display_links()</code> function displays links to open the Dfr-browser(s) in a tab in your web browser.</p>"},{"location":"modules/dfr-browser/#create-zipped-copies-of-your-visualizations-optional","title":"Create Zipped Copies of Your Visualizations (Optional)","text":"<p>This section zips up your dfr-browser visualizations for serving on a different machine or server. By default, browsers for all available models will be zipped. If you wish to zip only one model, change the <code>models</code> setting to indicate the name of the model folder (e.g. <code>'topics25'</code>). If you wish to zip more than one model, but not all, provide a list in square brackets (e.g. <code>['topics25', 'topics50']</code>). This section also includes instructions for downloading and running your dfrbrowser visualization(s) on a different machine (i.e., outside of the WE1S container system).</p>"},{"location":"modules/dfr-browser/#customize_dfrbrowseripynb","title":"<code>customize_dfrbrowser.ipynb</code>","text":"<p>The <code>customize_dfrbrowser.ipynb</code> notebook allows you to customize certain Dfr-browser settings. You can only customize 1 topic model at a time. You can customize the title, names and contact info for contributors, a description of the model, a list of custom metadata fields, the number of words to display in topic bubbles, the font size of topic labels, the number of documents to display in Topic View, and the topic labels. This notebook edits your Dfr-browser's <code>info.json</code> file. You can also just edit this file manually to make these customizations. For more information on how to do this, see Goldstone's documentation here: https://github.com/agoldst/dfr-browser#tune-the-visualization-parameters.</p>"},{"location":"modules/dfr-browser/#module-structure","title":"Module Structure","text":"<p> dfr_browser  \u2523  scripts  \u2503 \u2503 \u2523  create_dfrbrowser.py  \u2503 \u2503 \u2517  zip.py  \u2523  dfrb_scripts  \u2503 \u2523  bin  \u2503 \u2503 \u2523  prepare-data  \u2503 \u2503 \u2517  server  \u2503 \u2523  css  \u2503 \u2503 \u2523  bootstrap-theme.min.css  \u2503 \u2503 \u2523  bootstrap.min.css  \u2503 \u2503 \u2517  index.css  \u2503 \u2523  fonts  \u2503 \u2503 \u2523  glyphicons-halflings-regular.eot  \u2503 \u2503 \u2523  glyphicons-halflings-regular.svg  \u2503 \u2503 \u2523  glyphicons-halflings-regular.ttf  \u2503 \u2503 \u2517  glyphicons-halflings-regular.woff  \u2503 \u2523  img  \u2503 \u2503 \u2517 :material-gif: loading.gif  \u2503 \u2523  js  \u2503 \u2503 \u2523  d3-mouse-event.js  \u2503 \u2503 \u2523  dfb.min.js.custom  \u2503 \u2503 \u2523  utils.min.js  \u2503 \u2503 \u2517  worker.min.js  \u2503 \u2523  lib  \u2503 \u2503 \u2523  bootstrap.min.js  \u2503 \u2503 \u2523  d3.min.js  \u2503 \u2503 \u2523  query-1.11.0.min.js  \u2503 \u2503 \u2517  jszip.min.js  \u2503 \u2523  index.html  \u2503 \u2517 LICENSE  \u2523  create_dfrbrowser.ipynb  \u2523  customize_dfrbrowser.ipynb  \u2517 README.md</p>"},{"location":"modules/diagnostics/","title":"Diagnostics","text":""},{"location":"modules/diagnostics/#about-this-module","title":"About This Module","text":"<p>This notebook produces a modified version of the diagnostics visualisation on the MALLET website. A single model will be viewable as a web page called <code>index.html</code>. The notebook also produces a comparative visualisation file for multiple models called <code>comparison.html</code>. The main notebook for this module is <code>visualize_diagnostics.ipynb</code>.</p>"},{"location":"modules/diagnostics/#user-guide","title":"User Guide","text":"<p>This module assembles the MALLET diagnostics xml filse together with assets for a web-based visualization of the contents. Because it does not generate any information itself, it simply outputs a link to the visualization index file.</p>"},{"location":"modules/diagnostics/#create-diagnostics-visualizations","title":"Create Diagnostics Visualizations","text":"<p>This cell copies all of the diagnostics xml files from the diagnostics module directory and generates two web pages called <code>index.html</code> and <code>comparison.html</code>. Opening <code>index.html</code> on the public visualization port (a link is created by the notebook) launches the visualizations. Instructions for using the visualizations can be viewed by clicking \"About This Tool\" in the menu. The \"Model Comparison Tool\" menu item switches to the comparison view, from which the \"Individual Model Tool\" will take you back to the single-model visualization.</p> <p>Important</p> <p>In the Model Comparison Tool, one or two scatterplots may sometimes fail to load due to other browser activity. Usually doing a hard refresh of the page will allow them to load.</p>"},{"location":"modules/diagnostics/#zip-diagnostics","title":"Zip Diagnostics","text":"<p>This optional cell The second cell optionally creates a zip archive of the visualization, which is suitable for export, in the module directory.</p>"},{"location":"modules/diagnostics/#module-structure","title":"Module Structure","text":"<p> diagnostics  \u2523  css  \u2503 \u2503  bootstrap.min.css  \u2503 \u2503  all.min.css  \u2503 \u2503  styles.css  \u2503 \u2517  bootstrap.min.css  \u2523  js  \u2503 \u2523  bootstrap.min.js  \u2503 \u2523  d3.v3.min.js  \u2503 \u2523  jquery-3.4.1.slim.min.js  \u2503 \u2517  popper.min.js  \u2523  scripts  \u2503 \u2523  comparison_template.html  \u2503 \u2523  diagnostics.py  \u2503 \u2523  index_template.html  \u2503 \u2523  zip.py  \u2523  webfonts  \u2503 \u2523  fa-solid-900.woff2  \u2503 \u2523  fa-solid-900.woff  \u2503 \u2523  fa-solid-900.ttf  \u2503 \u2523  fa-solid-900.svg  \u2503 \u2523  fa-solid-900.eot  \u2503 \u2523  fa-regular-400.woff2  \u2503 \u2523  fa-regular-400.woff  \u2503 \u2523  fa-regular-400.ttf  \u2503 \u2523  fa-regular-400.svg  \u2503 \u2523  fa-regular-400.eot  \u2503 \u2523  fa-brands-400.woff2  \u2503 \u2523  fa-brands-400.woff  \u2503 \u2523  fa-brands-400.ttf  \u2503 \u2523  fa-brands-400.svg  \u2503 \u2517  fa-brands-400.eot  \u2523  xml  \u2523  README.md  \u2517   visualize_diagnostics.ipynb</p>"},{"location":"modules/export/","title":"Export","text":""},{"location":"modules/export/#about-this-module","title":"About This Module","text":"<p>The <code>export</code> module provides utilities for exporting data from a project or a project as a whole to a compressed tar archive. The tar format is preferred to the zip format for entire projects because it preserves file permissions. This is less important for exports of the data itself.</p> <p>The module has two notebooks: <code>export_project.ipynb</code> for exporting an entire project and <code>json_to_txt_csv.ipynb</code> for exporting files from the project <code>json</code> folder to a directory of plain text files with an accompanying metadata CSV file.</p>"},{"location":"modules/export/#user-guide","title":"User Guide","text":""},{"location":"modules/export/#export-project","title":"Export Project","text":"<p>This notebook provides the ability to export an entire project to a single file in the form of a .tar.gz archive. File size can be reduced by setting a list of folders for exclusion. Once the archive has been created, its location can optionally be recorded in a MongoDB database. (Eventually it will be possible to store the archive in the database, but this feature is not yet available.</p>"},{"location":"modules/export/#configuration","title":"Configuration","text":"<p>The notebook expects the following values in the Configuration cell:</p> <ul> <li><code>name</code>: The name of the project archive</li> <li><code>author</code>: The name of the author of the archive</li> <li><code>version</code>: The version number</li> <li><code>save_path</code>: The filepath where the archive will be save (including filename)</li> <li><code>exclude</code> (optional): List of folder paths to ignore. Paths should be relative to the project folder without a leading '/'.</li> <li><code>client</code> (optional): The url of the MongoDB client</li> <li><code>database</code> (optional): The name of a MongoDB database</li> <li><code>collection</code> (optional): The name of a MongoDB database</li> </ul> <p>If you are not working with MongoDB, leave these the <code>client</code>, <code>database</code>, and <code>collection</code> set to <code>None</code>.</p>"},{"location":"modules/export/#build-data-package","title":"Build Data Package","text":"<p>This cell instantiates the <code>ExportPackage</code> object and builds a Frictionless Data data package detailing the project's resources. If the project directory contains a <code>datapackage.json</code> and/or <code>README.md</code>, a datetime stamp will be added; otherwise, these files will be created.</p> <p>Once the data package is built, it is possible to access it with <code>export_package.datapackage</code>, and the <code>README</code> text can be accessed with <code>export_package.readme</code>.</p>"},{"location":"modules/export/#make-archive","title":"Make Archive","text":"<p>This cell creates the archive file. It can be run without modification.</p>"},{"location":"modules/export/#extract-archive","title":"Extract Archive","text":"<p>The last two cells can be used to extract an existing project archive file to a project folder.</p> <p>Before running the last cell set the following configurations:</p> <p><code>archive_file</code>: The path the archive file to be extracted <code>destination_dir</code>: The path to the project folder where the project will be extracted. If the folder does not exist, it will be created. <code>remove_archive</code>: By default, the archive file copied to the project folder (not the original one) will be deleted after it is extracted. If you wish to retain it, set <code>remove_archive=False</code>.</p>"},{"location":"modules/export/#json_to_txt_csv","title":"json_to_txt_csv","text":"<p>The WE1S workflows use JSON format internally for manipulating data. However, you may wish to export JSON data from a project to plain text files with a CSV metadata file for use with other external tools.</p> <p>This notebook uses JSON project data to export a collection of plain txt files \u2014 one per JSON document \u2014 containing only the document contents field or bag of words. Each file is named with the name of the JSON document and a <code>.txt</code> extension.</p> <p>It also produces a <code>metadata.csv</code> file. This file contains a header and one row per document with the document filename plus required fields.</p> <p>Output from this notebook can be imported using the import module by copying the <code>txt.zip</code> and <code>metadata.csv</code> from <code>project_data/txt</code> to <code>project_data/import</code>. However, it is generally not recommended to export and then reimport data, as you may lose metadata in the process.</p>"},{"location":"modules/export/#configuration_1","title":"Configuration","text":"<p>The default configuration assumes:</p> <ol> <li>There are JSON files in <code>project_data/json</code>.</li> <li>Each JSON has the required fields <code>pub_date</code>, <code>title</code>, <code>author</code>.</li> <li>Each JSON file has either:</li> <li>a <code>content</code> field, or</li> <li>a <code>bag_of_words</code> field created using the <code>import</code> module tokenizer.</li> </ol> <p>The following configurations are accepted:</p> <ul> <li><code>limit</code>: The number of files to export. Set to <code>0</code> to export all files.</li> <li><code>txt_dir</code>: The path to the directory where text files will be saved.</li> <li><code>metafile</code>: The path to the metadata CSV file (including filename) that will be saved.</li> <li><code>zipfile</code>: The path to the zip archive (including filename) that will be saved if <code>zip_output=True</code>.</li> <li><code>zip_output</code>: Whether or not to create a zip archive the exported plain text files. This option automatically deletes the plain text files after they are zipped.</li> <li><code>clear_cache</code>: If set to <code>True</code>, previous export contents in the <code>txt</code> directory, including metadata and zip files will be deleted before an export is started.</li> <li><code>txt_content_fields</code>: A list of JSON fields to be checked in order for data content. The first field encountered in a document will be used for the data export. Documents with no listed field will be excluded from export.</li> <li><code>csv_export_fields</code>: A list of JSON fields to be exported to the metadata file. Fields in this list will become the columns in the CSV file.</li> </ul>"},{"location":"modules/export/#export_1","title":"Export","text":"<p>This cell starts the export. It can be run without modification.</p>"},{"location":"modules/export/#export-features-tables","title":"Export Features Tables","text":"<p>If your data contains features tables (lists of lists containing linguistic features), you can use this cell to export features tables as CSV files for each document in your JSON folder. Set the <code>save_path</code> to a directory where you wish to save the CSV files.</p>"},{"location":"modules/export/#module-structure","title":"Module Structure","text":"<p> export  \u2523  scripts  \u2503 \u2523  export_package.py  \u2503 \u2517  json_to_txt_csv.py  \u2523   export_project.ipynb  \u2523   json_to_txt_csv.ipynb  \u2517  README.md</p>"},{"location":"modules/import/","title":"Import","text":""},{"location":"modules/import/#about-this-module","title":"About This Module","text":"<p>The <code>import</code> module is main starting point for using the WE1S Workspace project. Use this module to import data into your project. Imported data is stored in the <code>project_data</code> folder. The import pipeline attempts to massage all data into a collection of JSON files, stored in the <code>project_data/json</code> folder. These JSON files contain both data and metadata as a series of key-value pairs that conform to the JSON file format. The names of the keys (also called \"fields\") conform to the WE1S manifest schema and certain metadata required by the tools in the WE1S Workspace. After import, the text content of your data will be stored in the JSON files' <code>content</code> field. JSON files are human-readable text files, which you can open and inspect. However, it is very easy to corrupt their format. If you ever suspect that this may have happened, you can paste the text into a JSON validator like jsonlint.com to check for errors.</p> <p>The import notebook requires either a zip archive of your data or a connection to a MongoDB database where the data is stored. Zip archives may take one of the following forms:</p> <ol> <li>A zip archive containing plain text data and an accompanying CSV file with relevant metadata.</li> <li>A zip archive containing data already in JSON format.</li> <li>A zip archive of a Frictionless Data data package containing data already in JSON format.</li> </ol> <p>If your metadata does not contain the field names required by the WE1S workspace, the import module allows to map your metadata's field names onto the WE1S field names.</p>"},{"location":"modules/import/#user-guide","title":"User Guide","text":""},{"location":"modules/import/#setup","title":"Setup","text":"<p>This cell imports various Python modules and defines paths that will be used by the module. In most cases, you will not need to change the default settings.</p>"},{"location":"modules/import/#configuration","title":"Configuration","text":"<p>Configuration options are explained briefly below.</p> <ul> <li><code>zip_file</code>: The name of the zip archive containing your data. By default, the archive is called <code>import.zip</code>, but you can modify the filename. If the data is in plain text format, you must also prepare a <code>metadata.csv</code> file. Does not apply when importing from MongoDB (you can set it to <code>None</code>).</li> <li><code>metadata.csv</code>: The name of your metadata file if you are importing plain text data. By default, it is called <code>metadata.csv</code>, but you can change the name.</li> </ul> <p>Important</p> <p>The metadata file must have <code>filename</code>, <code>pub_date</code>, <code>title</code>, and <code>author</code> as its first four headers. You can include additional metadata fields after the <code>author</code> field. Does not apply when importing directly from JSON files or from MongoDB (you can set it to <code>None</code>).</p> <ul> <li><code>remove_existing_json</code>: Empty the json folder before importing. The default is <code>False</code>, so it is possible to add additional data on multiple runs.</li> <li><code>delete_imports_dir</code>: If set to <code>True</code>, the folder containing your <code>zip_file</code> and <code>metadata.csv</code> file will be deleted when the import is complete. Does not apply when importing from MongoDB (you can set it to <code>None</code>).</li> <li><code>delete_text_dir</code>: If set to <code>True</code>, the folder containing your imported plain text files will be deleted after they are converted to json format. Does not apply when importing directly from JSON files or from MongoDB (you can set it to <code>None</code>).</li> <li><code>data_dirs</code>: If you are importing data already in json format, you can specify a list of paths in your zip archive or Frictionless Data data package where the json files are located. Does not apply when importing from MongoDB (you can set it to <code>None</code>).</li> <li><code>title_field</code>: If you are importing data already in json format that does not contain a field named <code>title</code> you can map an existing field to this key by providing the name of the existing field here.</li> <li><code>author_field</code>: If you are importing data already in json format that does not contain a field named <code>author</code> you can map an existing field to this key by providing the name of the existing field here.</li> <li><code>pub_date_field</code>: If you are importing data already in json format that does not contain a field named <code>pub_date</code> you can map an existing field to this key by providing the name of the existing field here.</li> <li><code>content_field</code>: If you are importing data already in json format that does not contain a field named <code>content</code> you can map an existing field to this key by providing the name of the existing field here.</li> <li><code>dedupe</code>: If set to <code>True</code>, the script will check for duplicate files within the project that may have been created by importing data from multiple zip archives. Duplicate files will be given the extension <code>.dupe</code>. This option also changes the extension of json files containing empty <code>content</code> fields to <code>.empty</code>.</li> </ul> <p>Warning</p> <p>For very large projects (~100,000 or more documents), duplicate detection may take up to several hours to run and, depending on other traffic on the server, may cause a server error.</p> <ul> <li><code>random_sample</code>: If you wish to import a random sample of the data in your <code>zip_file</code>, specify the number of documents you wish to import.</li> <li><code>random_seed</code>: Specify a number to initialize the random sampling. This ensures reproducibility if you have to run the import multiple times. In most cases, the setting can be left as <code>1</code>.</li> <li><code>required_phrase</code>: A word or phrase which will be used to filter the imported data. Only documents that contain the <code>required_phrase</code> value will be imported to your project.</li> <li><code>log_file</code>: The path to the file where errors and deduping results are logged. The default is <code>import_log.txt</code> in the same folder as this notebook.</li> </ul> <p>If you are importing your data directly to MongoDB, rather than a project folder, configure your MongoDB <code>client</code>, your database as <code>db</code>, and the name of your <code>collection</code>. For the <code>client</code> setting you can simply enter <code>MONGODB_CLIENT</code> to use your project's configuration. If importing from MongoDB, the <code>query</code> setting should be a valid MongoDB query. Since MongoDB syntax can be difficult \u2014 especially for complex queries \u2014 you may wish to use the WE1S QueryBuilder to construct your query and then paste it into the configuration cell. For information on using the Query Builder with your data, see on Using the QueryBuilder below.</p>"},{"location":"modules/import/#prepare-the-workspace-for-file-import","title":"Prepare the Workspace for File Import","text":"<p>You should use this cell only if you are importing from a zip archive.</p> <p>This cell sets up an import task based on the configuration you have supplied in the previous cell. When you run this cell, it will indicate whether the setup process was successful. Once the task is set up, instructions are displayed for uploading your data and metadata files to the Workspace.</p>"},{"location":"modules/import/#perform-the-import","title":"Perform the Import","text":"<p>This cell starts the import process. A progress bar indicates the percentage of files imported. You may also receive error messages if there was a problem importing specific files. A log file will be generated, and you can inspect this file to see the nature of the errors. When the process is finished, instructions for tokenize the data are displayed. See the Tokenize the Data section below.</p>"},{"location":"modules/import/#mongodb","title":"MongoDB","text":"<p>This cell starts the import process directly from MongoDB. Assuming that the client, database, and collection have been configured correctly in the Configuration section, it should work automatically. A progress bar indicates the percentage of files imported. You may also receive error messages if there was a problem importing specific files. A log file will be generated, and you can inspect this file to see the nature of the errors. When the process is finished, instructions for tokenize the data are displayed. See the Tokenize the Data section below.</p> <p>For help with constructing MongoDB queries, you may wish to use the WE1S QueryBuilder to construct your query and then paste it into the configuration cell. For information on using the Query Builder with your data, see on Using the QueryBuilder below.</p>"},{"location":"modules/import/#tokenize-the-data","title":"Tokenize the Data","text":"<p>This cell is optional, but it can save time when performing tasks in other tools. Normally text analysis tools have to divide a text into countable \"tokens\" (most frequently words). This process is called tokenization. This cell allows you to pre-tokenize your data so that other tools do not need to take this step. It generates a dictionary of token-count pairs such as <code>{\"cat\": 3, \"dog\": 2}</code> for each of your JSON files. This dictionary is appended to the JSON file in the <code>bag_of_words</code> field.</p> <p>The import tokenizer offers two tokenization methods. The default method is strips all non-alphanumeric characters and then divides the text into tokens on white space. Alternatively, you can use the spaCy Natural Language Processing library to tokenize based on spaCy's language model. spaCy extracts linguistic <code>features</code> from your text, not only tokens but parts of speech and named entities. This is instrinsically slower and may require a lot of memory for large texts. To use WE1S's custom spaCy tokenizer, set <code>method='we1s'</code>. If your text has been previously processed by spaCy and there is a <code>features</code> table in your JSON file, the tokenizer will attempt to use it to build the <code>bag_of_words</code> dictionary. If you do not have a <code>features</code> table but would like to save one to your JSON files, configure <code>save_features_table=True</code>.</p>"},{"location":"modules/import/#using-the-querybuilder","title":"Using the QueryBuilder","text":"<p>The QueryBuilder is a simple web-based form that allows you to select metadata field names, operators such as \"is equal to\" or \"contains\", and values to match in the database. It can be used to generate very complex queries that are difficult to write by hand. To launch the QueryBuilder, open <code>query-builder/index.html</code>, configure a query in the form, and click \"Get Query to display the query you have configured. You can then copy the one-line query string into the import notebook's <code>query</code> setting.</p> <p>The QueryBuilder may not work if you open the web page on a sandboxed server. If you try to use it in this setting, you will receive a warning with the suggestion to download the <code>query-builder-bundle.zip</code> file. Download and extract this file to your local computer, and you can open <code>index.html</code> to run the QueryBuilder locally.</p> <p>By default, the QueryBuilder is configured to display common fields in the WE1S manifest schema. However, you may have metadata fields in your JSON files that are not part of the schema and will therefore not appear in the dropdown menu. If this is the case, you can easily configure the QueryBuilder for metadata fields since its configuration file is also in JSON format. Just open <code>query-builder/assets/config.js</code>. Copy a section between curly braces and rename the the <code>id</code> and <code>label</code> to your desired field name. The <code>type</code> should be \"string\", \"integer\", \"boolean\", or \"date\", depending on the type of value that occurs in that metadata category. There are a couple of examples that perform validation (such as for date format) that can be used as models for your own fields. The QueryBuilder is based on jQuery QueryBuilder. See its documentation for more sophisticated forms of customization. We recommend that you make a backup of the <code>schema.js</code> file before modifying it. If find that you have corrupted the format of the format, you can paste it into a JSON validator like jsonlint.com (omitting the <code>var schema =</code> at the beginning) to check for errors.</p>"},{"location":"modules/import/#module-structure","title":"Module Structure","text":"<p> import  \u2523  scripts  \u2503 \u2503 \u2523  import.py  \u2503 \u2503 \u2523  import_tokenizer.py  \u2503 \u2503 \u2517  timer.py  \u2523  query-builder  \u2503 \u2523  assets  \u2503 \u2503 \u2523  config  \u2503 \u2503 \u2503 \u2523  schema.js  \u2503 \u2503 \u2523  css  \u2503 \u2503 \u2503 \u2523  query-builder.default.min.css  \u2503 \u2503 \u2503 \u2517  tyles.css  \u2503 \u2503 \u2523  js  \u2503 \u2503 \u2503 \u2523  query-builder.standalone.min.js  \u2503 \u2503 \u2503 \u2517  builder.js  \u2503 \u2523  index.html  \u2503 \u2517  README.md  \u2523   import.ipynb  \u2523  query-builder-bundle.zip  \u2517  README.md</p>"},{"location":"modules/json-utilities/","title":"json_utilities","text":""},{"location":"modules/json-utilities/#about-this-module","title":"About This Module","text":"<p>This notebook provides a method of accessing the contents of a project's <code>json</code> folder. These folders can be quite large, and they will cause the browser to freeze if they are opened using the Jupyter notebook file browser. This notebook creates a <code>Documents</code> object with which you can call methods that list or read the contents of the files in the <code>json</code> folder. It also allows you to perform database-like queries on the contents to filter your results and to export the results to a zip archive.</p>"},{"location":"modules/json-utilities/#user-guide","title":"User Guide","text":"<p>The main notebook <code>json_utilities.ipynb</code> functions more like a tutorial than the notebooks of other modules. There are built-in examples, which you can work through and modify according to your data and research questions. You can also use <code>remove_fields.ipynb</code> to remove specific fields from json files.</p>"},{"location":"modules/json-utilities/#json_utilitiesipynb","title":"<code>json_utilities.ipynb</code>","text":""},{"location":"modules/json-utilities/#setup","title":"Setup","text":"<p>This cell imports Python libraries and scripts.</p>"},{"location":"modules/json-utilities/#view-metadata-fields-optional","title":"View Metadata Fields (Optional)","text":"<p>If you wish to perform a query on your documents, it can be helpful to know what metadata fields are available. The cell below will read the first 100 documents and extract the keys for each metadata field. Note that listed keys may not be available in all documents. If you think that your metadata is very inconsistent, you may want to run <code>docs.get_metadata_keys()</code> without start and end values. However, this can take a long time, so it is not recommended unless you have reason to think that there are large discrepancies across your collection.</p> <p>It is also possible to get the keys for a specific file with <code>docs.get_metadata_keys(filelist=['file1', 'file2', etc.])</code>. If you have already run something like <code>result = docs.get_file_list(0, 5)</code>, you can simply run <code>docs.get_metadata_keys(filelist=result)</code>.</p> <p>In the second cell, you can generate a table of your documents with <code>get_table()</code>. It takes a list of files and a list of fields as its arguments, as in the example below. Columns can be re-ordered, sorted, and filtered. However, it is recommended that you only supply a small number of columns. The bigger the table, the longer the lag time when you scroll.</p> <p>If you wish to save the table after you have sorted and/or filtered it, set the <code>filename</code> in the third cell and run the cell.</p>"},{"location":"modules/json-utilities/#performing-queries","title":"Performing Queries","text":"<p>Although many questions about your data can be answered by working with the table above, sometimes you may need to perform more  sophisticated database-like queries to filter the data in your project's json folder. The cell below provide an interface for performing these queries.</p> <p>A basic query is given in the form of a tuple with the syntax <code>(fieldname, operator, value)</code>. The <code>fieldname</code> is the name of the metadata field you wish to search. The <code>value</code> is the value you are looking for in the field, and the <code>operator</code> is the method by which you will evaluate the value. Here are the possible operators: <code>&lt;</code>, <code>&lt;=</code>, <code>=</code> (or <code>==</code>), <code>!=</code> (meaning \"not equal to\"), <code>&gt;</code>, <code>&gt;=</code>, <code>contains</code>. The last will match any value anywhere in the field. For greater power, you can use <code>regex</code> as the <code>operator</code> and a regex pattern as the <code>value</code>.</p> <p>Important</p> <p>The <code>fieldname</code> and <code>operator</code> must be enclosed in single quotes. The <code>value</code> must also be single quotes unless it is a number or Boolean (<code>True</code> or <code>False</code>).</p> <p>The <code>find()</code> method takes three arguments: a list of filenames, a query, and, optionally, a Boolean <code>lower_case</code> value. If <code>lower_case=True</code> the <code>value</code> data will be converted to lower case before it is evaluated. The default is <code>False</code>.</p> <p>In the cell below, we will get a list of the first 5 files (to keep things quick) and search for the ones that contain \"Politics\" in the document's <code>name</code> field.</p> <p>Note: There is a built-in timer class that can be used to time queries of long file lists. Its use is illustrated in the cell below, but it can be used to time any of the methods.</p>"},{"location":"modules/json-utilities/#performing-multiple-queries","title":"Performing Multiple Queries","text":"<p>You can pass multiple queries to the <code>find()</code> method by using a list of tuples. As you can see from the example below. The result will be every document that matches any of the queries in the list.</p>"},{"location":"modules/json-utilities/#adding-boolean-logic","title":"Adding Boolean Logic","text":"<p>It is possible to add more complex Boolean logic by passing a dictionary as the query with <code>'and'</code> or <code>'or'</code> as the key. The value should be a list of one or more tuples, or a list of dictionaries as shown in the examples.</p>"},{"location":"modules/json-utilities/#exporting-the-results-of-a-query","title":"Exporting the Results of a Query","text":"<p>You can save the documents found by your query to a zip file with the <code>export()</code> method. It takes a list of filenames and a path where you wish to save the zip file. A filename is sufficient if you wish to save it in the current folder.</p> <p>The <code>export()</code> method takes an optional <code>text_only</code> argument. Setting <code>text_only=True</code> will export only the <code>content</code> fields as plain text files.</p> <p>Here is an example in which you create a <code>Documents</code> object, get a file list, find files in the list that match your query, and export the results to a zip archive.</p> <p>The timer class is automatically applied to exports.</p>"},{"location":"modules/json-utilities/#remove_fieldsipynb","title":"<code>remove_fields.ipynb</code>","text":"<p>This notebook will remove specified field from all files in the <code>json</code> folder. It is intended primarily for creating sample data sets for testing, but we could consider documenting it fully and keeping it in the release version of the module.</p>"},{"location":"modules/json-utilities/#configuration","title":"Configuration","text":"<p>Configure a list of fields to remove. By default, the folder containing the json files is the project's <code>json</code> directory, but you can configure another folder (relative to the project root). Errors will be logged to a file saved with the name you configure.</p>"},{"location":"modules/json-utilities/#setup_1","title":"Setup","text":"<p>This cell loads your configurations and instantiates your removal job.</p>"},{"location":"modules/json-utilities/#remove-fields","title":"Remove Fields","text":"<p>This cell removes the fields from json files in the directory you have configured.</p>"},{"location":"modules/json-utilities/#module-structure","title":"Module Structure","text":"<p> import  \u2523  scripts  \u2503 \u2517  json_utilities.py  \u2523   json_utilities.ipynb  \u2523   remove_fields.ipynb  \u2517  README.md</p>"},{"location":"modules/metadata/","title":"Metadata","text":""},{"location":"modules/metadata/#about-this-module","title":"About This Module","text":"<p>The main notebook in this module, <code>topic_statistics_by_metadata.ipynb</code>, enables you to generate some basic statistics about document counts based on the metadata fields available in your project's JSON files.</p> <p>There is also a notebook (<code>scattertext.ipynb</code>) for generating visualisations using the Scattertext library.</p> <p>Since these notebooks, and other tools in the Workspace, make use of document metadata, this module also provides a utility notebook (<code>add_metadata.ipynb</code>) for adding metadata fields to your project's JSON files after the data has been imported.</p>"},{"location":"modules/metadata/#user-guide","title":"User Guide","text":""},{"location":"modules/metadata/#add_metadataipynb","title":"<code>add_metadata.ipynb</code>","text":"<p>This notebook can be used to add metadata to project JSON files from a CSV file after they have been imported into the project.</p> <p>The CSV file must have a <code>filename</code> column referencing files in the <code>json</code> folder. If this does not exist, the script will look for a <code>name</code> column, append <code>.json</code>, and attempt to use it as a filename.</p>"},{"location":"modules/metadata/#configuration","title":"Configuration","text":"<p>The only required configuration is the path to your metadata CSV file.</p>"},{"location":"modules/metadata/#load-the-metadata","title":"Load the Metadata","text":"<p>This cell loads the metadata file and performs other crucial setup functions. When it is finished, it displays the metadata in tabular format.</p>"},{"location":"modules/metadata/#add-the-metadata-to-project-json-files","title":"Add the Metadata to Project JSON files","text":"<p>This cell iterates through the metadata rows and adds the metadata field values to each file listed with a corresponding file in the JSON directory. If the metadata CSV does not have a filename listed, the notebook will attempt to create one from a <code>name</code> field. If a filename still cannot be found, the row will be skipped. Error messages are displayed if a filename could not be detected in the metadata CSV or if there is no corresponding file in the JSON folder.</p>"},{"location":"modules/metadata/#scattertextipynb","title":"<code>scattertext.ipynb</code>","text":"<p>This notebook uses Jason Kessler's Scattertext library to allow you to explore key terms in your corpus in relation to your documents' metadata. It does not require you to have topic modelled your data.</p> <p>Because Scattertext is under active development and has many more functions than are made available through this notebook, WE1S does not distribute it as part of the Workspace. Instead, it is downloaded and intalled in your environment when you run the first cell. You may receive a warning that Scattertext is already installed if you run the cell again. You can safely ignore this warning.</p> <p>Note that for the purpose of working with Scattertext, the original text is re-tokenized using slightly different rules from the WE1S preprocessor, so there may be some small discrepancies. By default, the WE1S standard stoplist in your project's MALLET module is applied.</p>"},{"location":"modules/metadata/#load-documents","title":"Load Documents","text":"<p>This cell loads the json documents for the entire collection. Configuration also takes place in this cell, so be sure to set the configurations as detailed below before running it. If you have run this cell previously and wish to use the same settings, you can skip this cell. The next cell will load your Documents dataframe from a stored copy called <code>documents_df.parquet</code>. If you wish to save the settings, this fille will be overwritten with the new dataframe, so make a backup if you wish to keep the old one.</p> <p>Loading can take a while, so, for experimentation, it is best to set <code>end</code> to a smaller number. This will limit the number of documents loaded. If you do not wish to start at the first document, set <code>start</code> to a higher number. Numbering is zero-indexed, so the first document is document 0.</p> <p>By default, only your documents' content is loaded, but you can load other metadata from your JSON files using the the <code>extra_fields</code> setting, which takes a dictionary of key-value pairs. The value is the name of the field you wish to use from your JSON files. It will be referenced by the equivalent key in Scattertext. So, if you want to use your <code>pub_date</code> field and refer to it as <code>date</code> in Scattertext, the dictionary would be {'date': 'pub_date'}. Note that WE1S data contains specific tags with hierarchical filepath like formats. If these are found, they will be parsed automatically. Tags such as \"media/news wires\" and \"media/news agency\" cannot be resolved to a single table column, so only the last tag will appear. If you are not using WE1S data but you have a field called <code>tags</code> (the values of which must be a dictionary), open <code>scattertext.py</code> and delete or comment out line 118: <code>tags = tags_to_dict(doc)</code>.</p> <p>If you wish to randomly sample a percentage of your corpus, set <code>random_sampling</code> to a number (without the \"%\" sign). If you do not wish to perform random sampling, set <code>random_sample=None</code>.</p>"},{"location":"modules/metadata/#save-the-dataframe-to-csv","title":"Save the Dataframe to CSV","text":"<p>By default, the output of the previous cell is a manipulatable dataframe. You can sort the columns by clicking on their labels or filter the table by clicking on the icons. If you wish to save the dataframe to a CSV file, configure a filename and then uncomment one of the lines below. The first will save the original dataframe and the second will save the dataframe after any sorting or filtering you have done.</p>"},{"location":"modules/metadata/#generate-document-counts-report","title":"Generate Document Counts Report","text":"<p>This cell provides a table of document counts for each field column beginning with the one configured for the <code>start_column</code> value. Each column provides the document counts for each metadata field in the data. The rows provide the document counts by value. This information will be used for configuring the following cells.</p>"},{"location":"modules/metadata/#build-a-corpus","title":"Build a Corpus","text":"<p>When the corpus is built, each document is parsed using spaCy, so this can take a while. For that reason, it is a good idea to set the limit to around 2000 documents or smaller.</p> <p>Before generating the corpus, the cell will automatically look for a previously-saved corpus file to speed loading time. If you have changed your <code>limit</code> or <code>field</code> settings, change the name of the <code>corpus_file</code> or set <code>from_file=False</code>. If you do not change the name of <code>corpus_file</code>, any previous corpus with that filename will be overwritten.</p> <p>Important</p> <p>A Scattertext corpus requires a <code>field</code> category corresponding to one of the column headings in the Document Counts Report above. The column must contain at least two non-zero. If you get an error, you may not have chosen a valid field.</p> <p>Results seem to be improved by using lemmas rather than the original tokens, but this can be changed with <code>use_lemmas=False</code>. The other options are more unpredicatable. The <code>entity_types_to_use</code> and <code>tag_types_to_use</code> lists allow you to specify entity and part of speech categories that should be retained in the analysis. Tokens not belonging to the types you specify will be excluded from the corpus. A list of the category abbreviations can be found in the spaCy documentation for named entities and part of speech tags. If you wish to use all the categories, set these values to <code>All</code>.</p> <p>You can also \"censor\" certain types, which replaces the original token it entity or part of speech abbreviation. Lastly, periods at the ends of tokens can be stripped if they have escaped spaCy's tokenizer.</p> <p>For convenience, here are lists of all entity and part of speech abbreviations, which you can use to copy and paste into the cell below.</p> <p>Named Entities</p> <p>\"PERSON\", \"NORP\", \"FAC\", \"ORG\", \"GPE\", \"LOC\", \"PRODUCT\", \"EVENT\", \"WORK_OF_ART\", \"LAW\", \"DATE\", \"TIME\", \"PERCENT\", \"MONEY\", \"QUANTITY\", \"ORDINAL\", \"CARDINAL\"</p> <p>Parts of Speech</p> <p>\"$\", \"``\", \"''\", \",\", \"-LRB-\", \"-RRB-\", \".\", \":\", \"ADD\", \"CC\", \"CD\", \"DT\", \"EX\", \"IN\", \"LS\", \"NFP\", \"NIL\", \"NNP\", \"NNPS\", \"PDT\", \"POS\", \"PRP\", \"PRP$\", \"RP\", \"SYM\", \"TO\", \"UH\", \"WDT\", \"WP\", \"WP$\", \"WRB\"</p> <p>You can perform quite a variety of configurations before generating your corpus:</p> <ul> <li><code>limit</code>: A number less than or equal to the <code>end</code> value in Load Documents</li> <li><code>field</code>: The metadata field you wish to explore</li> <li><code>corpus_file</code>: The name of the corpus file (no file extension is necessary)</li> <li><code>from_file</code>: If set to <code>True</code>, the notebook will look for a pre-existing corpus file before generating one from scratch.</li> <li><code>stoplist_path</code>: Path to a custom stop word file</li> <li><code>use_lemmas</code>: If set to <code>True</code>, the notebook will use lemmas (e.g. \"go\" for \"going\") instead of raw tokens. Lemmas are taken from spaCy's language model. The only modification is that the lemma of \"humanities\" is \"humanities\", not \"humanity\".</li> <li><code>entity_types_to_use</code>: A list of the above entity types to use. If the value <code>None</code> is provided, all types are used.</li> <li><code>entity_types_to_censor</code>: A list of entity types to \"censor\". This means that the tokens will be replaced by the entity label.</li> <li><code>tag_types_to_use</code>: A list of the above tag types to use. If the value <code>None</code> is provided, all types are used.</li> <li><code>tag_types_to_censor</code>: A list of tag types to \"censor\". This means that the tokens will be replaced by the tag label.</li> <li><code>strip_final_period</code>: Removes periods (full stops) from the end of tokens if they have been missed by the tokenizer.</li> </ul>"},{"location":"modules/metadata/#generate-terms-that-differentiate-the-collection-from-a-general-english-corpus","title":"Generate terms that differentiate the collection from a general English corpus","text":"<p>This cell provides a list of terms in your corpus that make it distinct from a general English corpus. We think that Scattertext uses the Brown University Standard Corpus of Present-Day American English (AKA the Brown Corpus) for comparison, but we have not been able to confirm this. As such, the results need to be interpreted with this uncertainty in mind.</p> <p>The <code>limit</code> configuration controls the number of terms displayed in the output.</p>"},{"location":"modules/metadata/#generate-terms-associated-with-a-field-value","title":"Generate terms associated with a field value","text":"<p>This cell provides a list of terms particularly associated with a particular metadata field value within your corpus. For the <code>score_query</code> configuration, supply one of the row values in the Generate Column Counts Report cell. The <code>score_label</code> can be a more human-readable or descriptive label for the value.</p>"},{"location":"modules/metadata/#generate-a-scattertext-visualization-of-term-associations","title":"Generate a Scattertext Visualization of Term Associations","text":"<p>This cell generates a Scattertext visualization, which provides a rich environment for exploring your corpus. The Scattertext visualization is saved as a single HTML file at the location you specify for <code>filename</code>. Be sure to set <code>limit</code> to the same number you used for generating the corpus.</p> <p>The <code>field_name</code> value should be taken from one of the row values in the Counts Report above. In the graph, the axis for this field will be labelled with the value you provide for <code>field_label</code>. The other axis will be labelled by the value you provide for <code>non_field_label</code>. You can also modify the width of the graph and supply an extra metadata category, which will be the name of a column in the Documents table above. The values for that category will be displayed above sample documents in the graph.</p> <p>The results can be filtered by minimum term frequency and pointwise mutual information (the higher the number the greater the requirement that terms co-occur in the same document).</p> <p>Warning</p> <p>Scattertext HTML files are enormous and can take a very long time to load in your browser. A file generated from a corpus of 2000 documents will generally take about 15 minutes. So plan accordingly!</p>"},{"location":"modules/metadata/#topic-statistics-by-metadata","title":"Topic Statistics by Metadata","text":"<p>This notebook extracts information form a model's <code>topic-docs.txt</code> file and combines it with the information in the documents' metadata fields to provide counts of the number of documents associated with specific metadata fields. Since MALLET automatically selects the top 100 documents in each topic, this is the basis for the data. The results can be viewed in pandas dataframes and saved to CSV files.</p> <p>The results may be visualised in a static bar chart (stacked or unstacked) or an interactive plotly bar chart for any metadata field. The visusalisations may be saved to static PNG files.</p>"},{"location":"modules/metadata/#configuration_1","title":"Configuration","text":"<p>Select one model to explore. Please run the next cell regardless of whether you change anything.</p> <p>If you are unsure of the name of your model, navigate to the <code>your_project_name/project_data/models</code> directory in your project, and choose the name of one of the subdirectories in that folder. Each subdirectory should be called <code>topicsn1</code>, where <code>n1</code> is the number of topics you chose to model, for example: <code>selection = 'topics100'</code>. Please follow this format exactly.</p> <p>Info</p> <p>In most cases, you should not need to change the <code>data_path</code> and <code>from_file</code> configurations. The <code>data_path</code> variable specifies the folder where the notebook's assets will be saved. Some cells below attempt to load assets from this data folder so that you do not need to re-run procedures if you have already run the cell once. If for some reason you wish to bypass loading data from a saved file, set <code>from_file=False</code> in the cell's configuration section.</p>"},{"location":"modules/metadata/#read-topic-docs-file","title":"Read Topic-Docs File","text":"<p>This cell loads the <code>topic-docs.txt</code> file data into a pandas dataframe.</p>"},{"location":"modules/metadata/#export-data-from-top-documents","title":"Export Data from Top Documents","text":"<p>Sometimes it can be useful to extract the text from the top documents in a topic in order to study them using a different tool. This cell allows you to export this content as plain text files save them as a zip archive. Before running the cell, configure the <code>save_path</code> with the path to the directory where you want to save the text files. If <code>topic_num</code> is set to <code>All</code>, the content of all documents will be exported. If you set it to a topic number, only the documents associated with that topic will be exported.</p> <p>This cell is not required for running subsequent cells.</p>"},{"location":"modules/metadata/#gather-collection-metadata","title":"Gather Collection Metadata","text":"<p>This cell gathers metadata for a list of JSON fields from the documents in the collection.</p> <p>In the dataframe output, tag attributes which exist but without subattributes have values of <code>1</code>; missing tag attributes have values of <code>0</code>. Tags with values like \"funding/US private college\" are represented on the table under the \"funding\" column with the value \"US private colleges\".</p> <p>You can drag column boundaries to change their width or column labels to re-order the columns. To sort the columns, click the column label (and click again to sort in reverse order). Click the filter icon to filter your data by column values.</p> <p>Note: This cell can take some time to run. If you have already run it once, it should read the metadata from a saved file. Set <code>from_file=False</code> to re-generate the data.</p>"},{"location":"modules/metadata/#save-to-csv","title":"Save to CSV","text":"<p>If you wish to save a copy of the output of Gather Collection Metadata to a CSV file, set <code>save_path</code> to a filename to save the CSV to. In the sample below, <code>N</code> represents the topic number, which is recommended so that you do not overwrite a file from a different model.</p> <p>By default, the CSV will reflect the table above after any modifications you make by filtering or sorting. If you wish to use the original table, set <code>use_original=True</code>.</p>"},{"location":"modules/metadata/#get-counts-by-field-values","title":"Get Counts by Field Values","text":"<p>This cell calculates document counts for each topic by field value using the <code>topic_docs_metadata</code> dataframe. In the configuration section, set <code>field</code> to the name of the metadata field you wish to calculate. Set <code>save_path</code> to a path to a CSV file if you wish to save the table. Set <code>use_original=True</code> if you wish the file to contain the original output. Otherwise, it will reflect any changes you make such as sorting and filtering.</p>"},{"location":"modules/metadata/#visualise-metadata-with-a-simple-bar-plot-static-version","title":"Visualise Metadata with a Simple Bar Plot (Static Version)","text":"<p>This cell generates a simple bar plot for visualising the data generated by the notebook.</p> <p>Set <code>fields</code> to a list of column headings in the <code>sums_and_means</code> dataframe. If you wish to display different names in the legend, provide a list of corresponding names for <code>legend_labels</code> (in the same order). You may adjust the <code>title</code>, <code>xlabel</code> (for the x-axis) and <code>ylabel</code> (for the y-axis) to describe the content of your data accurately.</p> <p>Since plots can be very cramped, you may want to look at a limited range of topics. To do this, modify the <code>start_topic</code> and <code>end_topic</code> values. You can also save space by creating a stacked plot with <code>stacked=True</code>. (The interactive plot in the next cell provides another option with pan and zoom features.)</p> <p>To save the plot a file, set <code>save_path</code> to a full file path, including the filename. The type of file is inferred from the extension. For instance, files ending in <code>.png</code> will be saved as PNG files and files ending in <code>.pdf</code> will be saved as PDF files. SVG format is also available.</p>"},{"location":"modules/metadata/#visualise-metadata-with-a-plotly-bar-plot-interactive-version","title":"Visualise Metadata with a Plotly Bar Plot (Interactive Version)","text":"<p>The interactive plot (using Plotly) takes the same settings as the static plot above, except the stacked mode is not available. However, because Plotly provides zoom and pan features, it is possible to display the entire range of topics in a single graph. Click and drag over the graph to zoom in on a location. Click the home icon in the Plotly toolbar to restore the default zoom level. Click on the boxes in the legend to show and hide specific categories. Double-click to restore the default display.</p> <p>You can download the plot as PNG file by clicking the camera icon in the Plotly toolbar. If you wish to save the interactive plot as a standalone web page, set the <code>save_path</code> to a full file path, including the filename ending in <code>.html</code>.</p>"},{"location":"modules/metadata/#generate-topic-doc-dictionary-optional-utility","title":"Generate Topic-Doc Dictionary (Optional Utility)","text":"<p>This cell generates a dictionary with topic numbers as keys and a list of filenames in each topic as the values. Individual topics can be expected with <code>topic_docs_dict[1]</code>, where \"1\" is the desired topic number. The dictionary can be saved as a JSON file.</p>"},{"location":"modules/metadata/#module-structure","title":"Module Structure","text":"<p> metadata  \u2523  data  \u2523  scripts  \u2503 \u2523  add_metadata.py  \u2503 \u2523  scattertext.py  \u2503 \u2517  topic_stats.py  \u2523   add_metadata.ipynb  \u2523  README.md  \u2523   scattertext.ipynb  \u2517   topic_statistics_by_metadata.ipynb</p>"},{"location":"modules/pyldavis/","title":"pyLDAvis","text":""},{"location":"modules/pyldavis/#about-this-module","title":"About This Module","text":"<p>pyLDAvis is a port of the R LDAvis package for interactive topic model visualization by Carson Sievert and Kenny Shirley.</p> <p>pyLDAvis is designed to help users interpret the topics in a topic model by examining the relevance and salience of terms in topics. Once a pyLDAvis object has been generated, many of its properties can be inspected as in tabular form as a way to examine the model. However, the main output is a visualization of the relevance and salience of key terms to the topics.</p> <p>pyLDAvis is not designed to use MALLET data out of the box. This notebook transforms the MALLET state file into the appropriate data formats before generating the visualization. The code is based on Jeri Wieringa's blog post Using pyLDAvis with Mallet and has been slightly altered and commented.</p> <p>The main module for this notebook is <code>create_pyldavis.ipynb</code>.</p>"},{"location":"modules/pyldavis/#user-guide","title":"User Guide","text":""},{"location":"modules/pyldavis/#settings","title":"Settings","text":"<p>The Settings cell defines paths and important variables used to create the pyLDAVis visualizations. The default settings will create a folder inside the pyLDAvis module for each topic model in your project (you can select specific models in the Configuration cell below). In most case, you will not need to change the default settings.</p>"},{"location":"modules/pyldavis/#configuration","title":"Configuration","text":"<p>Select models to create pyLDAvis visualizations for. Please run the next cell regardless of whether you change anything.</p> <p>By default, this notebook is set to create a pyLDAvis for all of the models in your project <code>models</code> directory. If you would like to select only certain models to produce a pyLDAvis for, make those selections in the next cell (see next paragraph). Otherwise leave the value for <code>selection</code> as <code>All</code>, which is the default.</p> <p>To produce pyLDAvis for a selection of the models you created, but not all: Navigate to the <code>your_project_name/project_data/models</code> directory in your project. Note the name of each subdirectory in that folder. Each subdirectory should be called <code>topicsn1</code>, where <code>n1</code> is the number of topics you chose to model. You should see a subdirectory for each model you produced. To choose which subdirectory/ies you would like to produce browsers for, change the value of <code>selection</code> in the cell below to a list of subdirectory names. For example, if you wanted to produce browsers for only the 50- and 75-topic models you created, change the value of <code>selection</code> below to this:</p> <p>Example:</p> <p><code>selection = ['topics50','topics75']</code></p> <p>Once you have configured your topic models, run the next cell in the section to ensure that all of the models you selected are detectable by the pyLDAvis module.</p>"},{"location":"modules/pyldavis/#add-metadata-and-labels-for-the-user-interface","title":"Add Metadata and Labels for the User Interface","text":"<p>This section is option and is for more advanced users. Skip this cell if you want to generate a basic pyLDAvis plot.</p> <p>The pyLDAvis plot can be customized to display metadata information in your project's json files. To do this, identify the index number for each model in the list displayed by the second Configuration cell and add the necessary information using the following lines in the next cell.</p> <p><pre><code>models[0]['metadata'] = 'pub'\nmodels[0]['ui_labels'] = [\n                'Intertopic Distance Map (via multidimensional scaling)',\n                'topic',\n                'publication',\n                'publications',\n                'tokens'\n            ]\n</code></pre> Remember that the first model is <code>models[0]</code>. Additional models would be <code>models[1]</code>, <code>models[2]</code>, etc.</p> <p>For the <code>metadata</code> line, enter the json field you would like to use. For instance, a basic pyLDAvis displays your model's documents. If you wish to display, publications, you could use the <code>pub</code> field.</p> <p>The <code>ui_labels</code> must be given in the following order:</p> <ol> <li>The title of the multidimensional scaling graph</li> <li>The type of unit represented by the graph circles</li> <li>The singular form of the unit represented in the bar graphs on the right</li> <li>The plural form of the unit represented in the bar graph on the right.</li> <li>The unit represented by the percentage in the Relevance display.</li> </ol> <p>The example above indicates that the model will represent a map of intertopic distances in which each topic will show the distribution of publications, as represented by the percentage of topic tokens in the publication.</p> <p>Hint</p> <p>If you are unsure what to put, you do not have to assign <code>ui_labels</code>. A visualization will still be generated but may not have appropriate labels for the type of metadata you are using.</p>"},{"location":"modules/pyldavis/#generate-visualizations","title":"Generate Visualizations","text":"<p>This cell generates the pyLDAvis visualizations for all selected topic models. The output is a set of links to all pyLDAvis visualizations in your project folder. See the next cell if you wish to make them public.</p> <p>Since this cell can take some time to run, the output is captured instead of shown as the script is processing. Run the following <code>output.show()</code> cell when it is finished to check that everything ran as expected.</p>"},{"location":"modules/pyldavis/#create-zipped-copies-of-your-visualizations-for-export","title":"Create Zipped Copies of your Visualizations for Export","text":"<p>This section allows you to create zip archives of your pyLDAvis visualizations for export. By default, visualizations for all available models will be zipped. If you wish to zip only one model, change the <code>models</code> setting to indicate the name of the model folder (e.g. <code>'topics25'</code>). If you wish to zip more than one model, but not all, provide a list in square brackets (e.g. <code>['topics25', 'topics50']</code>).</p>"},{"location":"modules/pyldavis/#access-pyldavis-data-attributes-optional","title":"Access pyLDAvis Data Attributes (Optional)","text":"<p>pyLDAvis generates a number of useful variables which it can be helpful for understanding the data underlying the visualization or for use in other applications. These variables can be accessed via the <code>vis</code> object created in Generate the Visualizations section. You can view the data by calling <code>show_attribute()</code> with the appropriate attribute configured. Possible attributes are <code>model_state</code> (a matrix o the models state file), <code>hyperparameters</code>, <code>alpha</code>, <code>beta</code>, <code>doc_lengths</code> (document lengths), <code>phi</code> (the matrix of topic-term distributions), <code>phi_df</code> (a pivoted dataframe of phi), <code>theta</code> (the matrix of document-topic), <code>theta_df</code> (a pivoted dataframe of theta), <code>vocab</code> (a matrix of term frequencies). Further information about these attributes can be found in the discussion of Jeri Wieringa's blog post Using pyLDAvis with Mallet.</p> <p>You can restrict the number of lines shown by modifying the <code>start</code> and <code>end</code> settings. If you wish to save the result to a file, set the <code>save_path</code>. Tabular data should be saved to a csv file; everything else can be plain text.</p>"},{"location":"modules/pyldavis/#module-structure","title":"Module Structure","text":"<p> pyldavis  \u2523  scripts  \u2503 \u2523  PyLDAvis.py  \u2503 \u2523  PyLDAvis_custom.js  \u2503 \u2517  zip.py  \u2523 jupyter-jupyter-logo: create_pyldavis.ipynb  \u2517  README.md</p>"},{"location":"modules/topic-bubbles/","title":"Topic Bubbles","text":""},{"location":"modules/topic-bubbles/#about-this-module","title":"About This Module","text":"<p>This module creates a topic bubbles visualization from dfr-browser data generated in the dfr-browser notebook or from model data generated in the topic modeling notebook. This module uses scripts originally written by Sihwa Park for the WE1S project. For more information on Park's script, see Park's topic bubbles Github repo and the <code>README.md</code> located in this module's <code>tb_scripts</code> folder.</p>"},{"location":"modules/topic-bubbles/#notebooks","title":"Notebooks","text":"<p><code>create_topic_bubbles.ipynb</code> is the main notebook for the module.</p>"},{"location":"modules/topic-bubbles/#user-guide","title":"User Guide","text":""},{"location":"modules/topic-bubbles/#settings","title":"Settings","text":"<p>The Settings cell defines paths and important variables used to create a topic bubbles visualization. The default settings will create a folder inside the topic_bubbles module for each topic model in your project or for a selection of models. In most cases, you will not need to change the default settings.</p>"},{"location":"modules/topic-bubbles/#create-topic-bubbles-using-dfr-browser-metadata","title":"Create Topic Bubbles Using Dfr-Browser Metadata","text":"<p>If you ran the <code>dfr-browser</code> module to create dfr-browser visualizations for your models, the next cells will import data produced via that module into the <code>topic_bubbles</code> module.</p> <p>If you did not run the <code>dfr_browser</code> module, skip to the next section: Create Topic Bubbles without Dfr-Browser Metadata.</p> <p>By default, this notebook is set to create Topic Bubbles visualizations the same models you produced Dfr-browsers for in the <code>dfr_browser</code> module. This means that the <code>selection</code> variable below is set to its default value of <code>All</code> (<code>selection = 'All'</code>). If you would like to select only certain models to produce Topic Bubbles visualizations, make those selections in the next cell. Otherwise leave the value in the next cell set to <code>All</code>. You must run this cell regardless of whether you change anything.</p> <p>To produce topic bubbles for a selection of models: Navigate to the <code>modules/dfr_browser</code> directory and look for subdirectories titled <code>topicsn1</code>, where <code>n1</code> is the number of topics you chose to model. You should see a subdirectory for each browser you produced. To choose which subdirectory/ies you would like to produce, change the value of <code>selection</code> in the cell below to a list of subdirectory names. For example, if you wanted to produce browsers for only the 50- and 75-topic models you created, change the value of <code>selection</code> below to <code>selection = ['topics50', 'topics75']</code>.</p>"},{"location":"modules/topic-bubbles/#create-topic-bubbles-without-dfr-browser-metadata","title":"Create Topic Bubbles without Dfr-Browser Metadata","text":"<p>If you have not yet created Dfr-browsers for this project, run the following cells to create your Topic Bubbles visualization.</p> <p>By default, this notebook is set to create Topic Bubbles visualizations the same models you produced Dfr-browsers for in the <code>dfr_browser</code> module. This means that the <code>selection</code> variable below is set to its default value of <code>All</code> (<code>selection = 'All'</code>). If you would like to select only certain models to produce Topic Bubbles visualizations, make those selections in the next cell. Otherwise leave the value in the next cell set to <code>All</code>. You must run this cell regardless of whether you change anything.</p> <p>To produce topic bubbles for a selection of models: Navigate to the <code>modules/dfr_browser</code> directory and look for subdirectories titled <code>topicsn1</code>, where <code>n1</code> is the number of topics you chose to model. You should see a subdirectory for each browser you produced. To choose which subdirectory/ies you would like to produce, change the value of <code>selection</code> in the cell below to a list of subdirectory names. For example, if you wanted to produce browsers for only the 50- and 75-topic models you created, change the value of <code>selection</code> below to <code>selection = ['topics50', 'topics75']</code>.</p>"},{"location":"modules/topic-bubbles/#create-files-needed-for-topic-bubbles","title":"Create Files Needed for Topic Bubbles","text":""},{"location":"modules/topic-bubbles/#select-models-for-which-you-would-like-to-create-visualizations","title":"Select Models for Which You Would Like to Create Visualizations","text":"<p>By default, this notebook is set to create Topic Bubbles visualizations the same models you produced Dfr-browsers for in the <code>dfr_browser</code> module. This means that the <code>selection</code> variable below is set to its default value of <code>All</code> (<code>selection = 'All'</code>). If you would like to select only certain models to produce Topic Bubbles visualizations, make those selections in the next cell. Otherwise leave the value in the next cell set to <code>All</code>. You must run this cell regardless of whether you change anything.</p> <p>To produce topic bubbles for a selection of models: Navigate to the <code>modules/dfr_browser</code> directory and look for subdirectories titled <code>topicsn1</code>, where <code>n1</code> is the number of topics you chose to model. You should see a subdirectory for each browser you produced. To choose which subdirectory/ies you would like to produce, change the value of <code>selection</code> in the cell below to a list of subdirectory names. For example, if you wanted to produce browsers for only the 50- and 75-topic models you created, change the value of <code>selection</code> below to <code>selection = ['topics50', 'topics75']</code>.</p> <p>The <code>get_model_state()</code> function in the second cell grabs the filepaths of model subdirectories in order to visualize and their state and scaled files. Optionally, you can instead set values for <code>subdir_list</code>, <code>state_file_list</code>, and <code>scaled_file_list</code> manually in the third cell.</p> <p>The <code>create_topicbubbles()</code> function creates the files needed for topic bubbles, using the model state and scaled files for all selected modelsand Goldstone's prepare_data.py script (to produce the dfr-browser files needed for topic bubbles). It prints output from Goldstone's prepare_data.py script to the notebook cell.</p>"},{"location":"modules/topic-bubbles/#create-zipped-copies-of-your-visualizations-optional","title":"Create Zipped Copies of Your Visualizations (Optional)","text":"<p>This section zips up your topic bubbles visualizations for serving on a different machine or server. By default, browsers for all available models will be zipped. If you wish to zip only one model, change the <code>models</code> setting to indicate the name of the model folder (e.g. <code>'topics25'</code>). If you wish to zip more than one model, but not all, provide a list in square brackets (e.g. <code>['topics25', 'topics50']</code>). This section also includes instructions for downloading and running your topic bubble visualization(s) on a different machine (i.e., outside of the WE1S container system).</p>"},{"location":"modules/topic-bubbles/#module-structure","title":"Module Structure","text":"<p> topic_bubbles  \u2523  scripts  \u2503 \u2523  create_dfrbrowser.py  \u2503 \u2523  create_topic_bubbles.py  \u2503 \u2523  zip.py  \u2523  tb_scripts  \u2503 \u2523  css  \u2503 \u2503 \u2517  style.css  \u2503 \u2523  data  \u2503 \u2503 \u2517  config.json  \u2503 \u2523  img  \u2503 \u2503 \u2523  screenshot.png  \u2503 \u2503 \u2517  we1s_logo.png  \u2503 \u2523  js  \u2503 \u2503 \u2523 d3-mouse-event.js  \u2503 \u2503 \u2523 script.js  \u2503 \u2503 \u2523 utils.min.js  \u2503 \u2503 \u2517 worker.min.js  \u2503 \u2523  lib  \u2503 \u2523  index.html  \u2503 \u2523  LICENSE  \u2503 \u2517  README.md  \u2523   create_topic_bubbles.ipynb  \u2517  README.md</p>"},{"location":"modules/topic-modeling/","title":"Topic Modeling","text":""},{"location":"modules/topic-modeling/#about-this-module","title":"About This Module","text":"<p>This module uses MALLET to topic model project data. Prior to modelling, the notebooks extract word vectors from the project's JSON files into a single <code>doc_terms.txt</code> file, which is then imported into MALLET. Stop words are stripped at this stage. By default, the WE1S Standard Stoplist contained in the <code>scripts</code> folder is used for this process. After modelling is complete, there is an option to create a scaled data file for use in visualisations such as <code>dfr-browser</code> and <code>pyLDAvis</code>.</p> <p>The main notebook for this module is <code>model_topics.ipynb</code>.</p>"},{"location":"modules/topic-modeling/#user-guide","title":"User Guide","text":"<p>This notebook performs topic modelling with MALLET by providing an easy-to-configure interface for the two basic steps, importing your data to MALLET and training your topics. The WE1S project performs a preprocessing step (creating the file to input to MALLET) before the import step and a post-processing step (scaling) after training. These steps are explained below. Both the preprocessing and the postprocessing steps are optional.</p> <p>These instructions provide a step-by-step guide to using the notebook, once cell at a time. Each cell is referenced here by its title.</p> <p>Note that when importing data to MALLET or creating topic models, files with the same name will be over-written. If you use different filenames, files from any previous runs will not be deleted.</p>"},{"location":"modules/topic-modeling/#settings","title":"Settings","text":"<p>The purpose of this cell is to load some external Python code and define some commonly used file paths. If you encounter errors in any of the subsequent cells, it is worth re-running the Settings cell to check that all variables have been defined and all external code has been loaded.</p> <p>In general, you can run this cell without changing anything. You are most likely to want to customise the following settings: <code>log_file</code>, <code>language_model</code>, <code>stoplist_file</code>. The <code>log_file</code> is a text file that lists errors generated when you run the Create File for Importing to MALLET cell. By default, it is saved to your <code>models</code> directory, but you can save it elsewhere if you wish. The <code>language_model</code> is the spaCy language model to use when tokenising your text if you run Create File for Importing to MALLET without a pre-tokenised collection of documents. More information on this is given in the instructions for Create File for Importing to MALLET below. The <code>stoplist_file</code> is the full path to the file containing stop words (words to be omitted from your model). The default setting points to a copy of the WE1S Standard Stoplist stored in the module's <code>scripts</code> folder. If you wish to use a different stoplist, set <code>stoplist_file</code> to the full path to your stoplist's location (including the filename).</p>"},{"location":"modules/topic-modeling/#create-file-for-importing-to-mallet","title":"Create File for Importing to MALLET","text":"<p>MALLET has two methods of importing data: from a flat directory of plain text files or a single file with one document per line. By default, the WE1S project uses the latter method. Create File for Importing to MALLET is a preprocessing step that collects data from your project's JSON files and assembles it into a doc_terms file, which is saved as <code>models/doc_terms.txt</code> in you project folder.</p> <p>Hint</p> <p>If you are running a model on a directory of plain text documents and you want to process them purely with MALLET, you can skip this cell.**</p> <p>Note that the file that is produced is a plain-text file containing 1 document in the corpus per line. To produce what we call the doc_terms file, each term in a document is counted, and the term is then written however many times it occurs in the document to the document's row in the doc_terms file. For example, if the word \"humanities\" occurs twice in a document, the row in the doc_terms file will read \"humanities humanites\". The document's row in the doc_terms file is a string of repeated terms like this, separated by spaces (what is sometimes called a bag of words).</p> <p>The prepare MALLET import script (<code>scripts/prepare_mallet_import.py</code>) creates a <code>PrepareMalletImport</code> object that handles the creation of a text file to be imported by MALLET. Although a <code>PrepareMalletImport</code> object can be created on a document with <code>mimport=PrepareMalletImport(0, 'manifest_path.json', 'mallet_import.txt', model='en_core_web_sm', stoplist='we1s_standard_stoplist.txt')</code>, it will typically be used with a directory of json files. This function loops through a sorted list of json files in the directory, creates a <code>MalletImport</code> object from each one, and saves its bag of words as a row in the <code>doc_terms.txt</code> file. This is the file that will be imported by MALLET.</p> <p>If stop words are to be filtered prior to topic modelling, the <code>prepare_data()</code> function in <code>prepare_mallet_import.py</code> should be fed a stoplist file. If you do not want to strip stop words, give it an empty file.</p> <p><code>prepare_mallet_import.py</code> first looks for a <code>bag_of_words</code> field in your JSON files. A <code>bag_of_words</code> is a pre-packaged set of words already counted. If the field is not present, it will next look for a <code>features</code> field, extract the tokens from that field, and generate a bag of words. In both cases, the tokens are assumed to have been previously tokenised using the WE1S preprocessor.</p> <p>If neither the <code>bag_of_words</code> nor <code>features</code> field is present, the script will attempt to tokenise the text in the <code>content</code> field on the fly using a slimmed-down version of the WE1S preprocessor. Note that this process will necessarily be slower and can take a long time for large projects. Tokenisation uses the Python spaCy package, which predicts linguistic features based on a language model. If spaCy is called to do the tokenisation, it will use the language model your designated in Settings in most cases, <code>en_core_web_sm</code> will be sufficient. Note that the language model must be installed in your environment for this process to work.</p> <p>Normally you will run Create File for Importing to MALLET without changing any of the settings. When it finishes, you will see a preview of the beginning of the <code>doc_terms.txt</code> file. By default, five rows will be displayed, with each row clipped at 200 characters. You can change these settings in the final line of the cell, or remove them if you wish to display the whole file (not recommended in a Jupyter notebook). You can also navigate to <code>models/doc_terms.txt</code> and download or open the file to inspect it. Each row in the <code>doc_terms.txt</code> file is one document in your corpus, and each row lists the document's filename, its index number, and its bag of words.</p>"},{"location":"modules/topic-modeling/#setup-mallet","title":"Setup MALLET","text":"<p>In the first cell, you configure a list of models you wish to run. Models are listed by the number of topics you select. For instance, if you wish to run three models of 25, 50, and 100 topics each, you should set <code>num_topics = [25, 50, 100]</code>.</p> <p>The second cell instantiates a <code>Mallet</code> object (referenced as <code>mallet</code> and creates subdirectories in your projects <code>models</code> folder for each model you listed.</p> <p>Important</p> <p>The <code>Mallet</code> object is pre-configured with MALLET settings used by the WE1S project. These settings are given below.</p> <ul> <li>`import_file_path=import_file_path</li> <li>`import_sources='file'</li> <li><code>num_iterations=1000</code></li> <li><code>optimize_interval=10</code></li> <li><code>use_random_seed=True</code></li> <li><code>random_seed=10</code></li> <li><code>keep_sequence=True</code></li> <li><code>preserve_case=False</code></li> <li>`token_regex=\"\\S+\"</li> <li><code>remove_stopwords=False</code></li> <li><code>extra_stopwords=None</code></li> <li><code>stoplist_file=None</code></li> <li><code>generate_diagnostics=True</code></li> </ul> <p>You can list the values for any of these settings with <code>print(mallet.import_file_path)</code>, <code>print(mallet.random_seed)</code>, etc.</p> <p>The first two options set the data source to be a file read from the location of the <code>import_file_path</code> configured in Settings. If a setting is <code>False</code> or <code>None</code>, it is not used by MALLET for importing data or training the models. In addition, <code>random_seed</code> is ignored if <code>use_random_seed</code> is <code>False</code>. Because WE1S input is pre-tokenised and has stop words removed, <code>remove_stopwords</code> is set to <code>False</code> and the <code>token_regex</code> setting just splits the doc_terms file on whitespace between words.</p> <p>Once you have run this cell, you are ready to begin importing your data to MALLET. However, you may need to adjust MALLET's configuration as described below.</p>"},{"location":"modules/topic-modeling/#custom-configuration","title":"Custom Configuration","text":"<p>Once the setup is complete,  you can change any of the MALLET settings below with commands like <code>mallet.optimize_interval = 11</code> (this model goes to 11!). You can use most arguments available in MALLET on the command line (see MALLET's documentation), but replace hyphens with underscores. For example, reference MALLET's <code>--num-iterations</code> argument in this notebook with <code>mallet.num_iterations</code>.</p> <p>Set you custom configurations in the cell in this section. For instance, if you wish to change the <code>optimize_interval</code> setting to '11', add <code>mallet.optimize_interval = 11</code> (this model goes to 11!). Another example with <code>num_iterations = 1000</code>.</p>"},{"location":"modules/topic-modeling/#importing-plain-text-files","title":"Importing Plain Text files","text":"<p>One common use case for custom configurations is if you wish to import data from a directory of plain text files. You can do this with <code>mallet.import_source='path_to_directory'</code>. You can also choose MALLET's default tokenisation and stop words instead of the WE1S tokenisation algorithm and stoplist using <code>mallet.token_regex=None</code> and <code>mallet.remove_stopwords=True</code>. These configurations are provided for convenience in the cell below. You just have to uncomment them and run the cell.</p> <p>Note that using plain text files that just contain document contents (and not metadata) as your data source means that certain visualisation tools like Dfr-Browser and Topic Bubbles, which require metadata, will not be usable. Therefore, this method is not recommended. If you wish to generate these visualisations, it is best to use the import module to import your data into your project's <code>json</code> folder first.</p>"},{"location":"modules/topic-modeling/#import-data-to-mallet","title":"Import Data to MALLET","text":"<p>You can probably simply run this cell as is, and the import process will begin. It may take a long time if your collection is large.</p> <p>By default, the topic list you supplied in Setup MALLET will be used. However, if, for instance, you wish to import data for only models 25 and 50, you can also provide a topic list here by typing <code>mallet.import_models([25, 50])</code>.</p> <p>This cell generates a MALLET command and uses it to call MALLET. If you run into a problem and you wish to see the MALLET command, create a new cell and run <code>print(mallet.import_command)</code>.</p> <p>Once the import process is complete, you are ready to begin training your models.</p>"},{"location":"modules/topic-modeling/#train-models","title":"Train Models","text":"<p>As with the previous cell, you can probably simply run this cell as is. Likewise, if you do not wish to train the models specified in Setup MALLET, you can supply a list of models here by typing <code>mallet.train_models([25, 50])</code>.</p> <p>When the training begins, MALLET gives continuous feedback with each iteration of the modelling process. By default, this feedback is hidden, and a progress bar indicates how close the model is to completion. You may wish to change this behaviour with one of the following settings:</p> <ul> <li><code>mallet.train_models(progress_bar=False)</code>: Display a plain text progress indicator every 10%.</li> <li><code>mallet.train_models(capture_output=True)</code>: Capture the output and display it only when training is complete. This is useful for job that takes a long time because it allows you to close the window.</li> <li><code>mallet.train_models(log_file='path_to_mallet_log.txt')</code>: Save the output to a log file at the path specified. This is useful if you wish to save a record of MALLET's feedback.</li> </ul> <p>If you run into a problem, you can inspect the last MALLET command by creating a new cell and running <code>print(mallet.train_command)</code>.</p>"},{"location":"modules/topic-modeling/#scale-topics","title":"Scale Topics","text":"<p>This cell uses Multidimensional Scaling (MDS) to adjust the topic weights for use in visualisation tools such as Dfr-Browser, pyLDAvis, and Topic Bubbles. These modules will not work properly if you do not perform scaling by running this cell. The generated scaling information is stored as <code>topic_scaled.csv</code> in the model's directory.</p> <p>By default, scaling files will be generated for the models you configured in Setup MALLET above. If you wish to specify which models to scale in this cell, replace <code>num_topics</code> with a list of topic desired topic numbers (e.g. <code>[50, 100]</code>) in the code below.</p>"},{"location":"modules/topic-modeling/#module-structure","title":"Module Structure","text":"<p> topic_modeling  \u2523  scripts  \u2503 \u2523  scale_topics.py  \u2503 \u2523  timer.py  \u2503 \u2523  mallet.py  \u2503 \u2523  prepare_mallet_import.py  \u2503 \u2523  slow.py  \u2503 \u2523  timer.py  \u2503 \u2517  we1s_standard_stoplist.txt  \u2523   model_topics.ipynb  \u2523  README.md</p>"},{"location":"modules/utilities/","title":"Utilities","text":""},{"location":"modules/utilities/#about-this-module","title":"About This Module","text":"<p>This module is really a container for miscellaneous \"helper\" notebooks that can be used to interact with the data in the project. These notebooks should be more or less self-documenting, so there is no user guide here.</p> <p>There are currently two notebooks:</p> <ul> <li><code>clear_caches.ipynb</code>: Empties or deletes files and folders and clears the output Jupyter notebooks.</li> <li><code>zip_folder.ipynb</code>: Create a zip or tar.gz archive of any folder in the project.</li> </ul>"},{"location":"modules/utilities/#module-structure","title":"Module Structure","text":"<p> utilities  \u2523   export_project.ipynb  \u2523  README.md  \u2517   zip_folder.ipynb</p>"}]}